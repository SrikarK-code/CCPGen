# Model configuration
model:
  encoder:
    _target_: byprot.models.encoder.GVPTransEncoder
    d_model: 512
  decoder:
    _target_: byprot.models.lm.modules.dplm_adapter.DPLMWithAdapterConfig
    hidden_size: 768
    num_hidden_layers: 12
    num_attention_heads: 12
  classifier_type: 'mlp'  # or 'transformer'
  diffmap_dim: 50
  classifier_hidden_dims: [256, 256]
  classifier_dropout: 0.1
  classifier_num_layers: 4  # for transformer
  classifier_num_heads: 8  # for transformer

# Training configuration
batch_size: 32
learning_rate: 1e-4
num_epochs: 50
device: 'cuda'

# Generation configuration
num_generated_samples: 5
max_iter: 50
temperature: 1.0

# Data configuration
train_data_path: 'path/to/train/data'
val_data_path: 'path/to/val/data'
test_data_path: 'path/to/test/data'
