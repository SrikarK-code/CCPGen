{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GxlZK1SHkgl9",
        "b-njBgLzltjL",
        "X-e-EUQyklC2",
        "ht7z5O8ilEnK",
        "PBEPeCXzk3zn",
        "zOjxMpSTljVi",
        "21dEMxSOw1ex",
        "JsBKL442wzXI",
        "lAAHts2YyYxk"
      ],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## workflow"
      ],
      "metadata": {
        "id": "GxlZK1SHkgl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Input: scVI embeddings of cell states\n",
        "b) Output: Generated protein sequences\n",
        "\n",
        "Process Flow:\n",
        "\n",
        "scVI embeddings → UNetModel → Flow Matching → Protein Decoder → Protein Sequence\n",
        "\n",
        "1. Input Preparation:\n",
        "   - Start with scVI (single-cell Variational Inference) embeddings of cell states.\n",
        "   - These embeddings are high-dimensional vectors (e.g., 128 dimensions) representing cellular gene expression profiles.\n",
        "   - The scVI embeddings are generated using a separate scVI model trained on single-cell RNA sequencing data.\n",
        "\n",
        "2. Model Architecture Overview:\n",
        "   The system consists of several key components:\n",
        "   a) UNetModel: The core generative model\n",
        "   b) FlowMatchingTrainer: Manages the flow matching process\n",
        "   c) ProtT5Encoder: Encodes protein sequences (for training)\n",
        "   d) ProtT5Decoder: Decodes latent representations to protein sequences\n",
        "\n",
        "3. UNetModel Detailed Architecture:\n",
        "   3.1. Initialization:\n",
        "   - The UNetModel is initialized with parameters like input/output channels, number of ResBlocks, attention resolutions, etc.\n",
        "   - Key components are created: time embedder, input blocks, middle block, output blocks.\n",
        "\n",
        "   3.2. Time Embedding:\n",
        "   - Function: timestep_embedding\n",
        "   - Converts a scalar timestep to a high-dimensional vector using sinusoidal functions.\n",
        "   - This embedding is further processed through a small MLP (self.time_embed).\n",
        "\n",
        "   3.3. Input Blocks:\n",
        "   - A series of TimestepEmbedSequential modules, each containing:\n",
        "     a) ResBlock: Combines feature maps with time embeddings\n",
        "     b) AttentionBlock or SpatialTransformer: For self-attention mechanisms\n",
        "     c) Downsample: Reduces spatial dimensions (if applicable)\n",
        "\n",
        "   3.4. Middle Block:\n",
        "   - Contains ResBlocks and Attention mechanisms for global reasoning.\n",
        "\n",
        "   3.5. Output Blocks:\n",
        "   - Mirror the input blocks, but with Upsample layers instead of Downsample.\n",
        "   - Use skip connections from input blocks.\n",
        "\n",
        "   3.6. Final Output Layer:\n",
        "   - Normalization followed by a convolution to produce the output channels.\n",
        "\n",
        "4. Detailed Component Breakdown:\n",
        "   4.1. ResBlock:\n",
        "   - Residual block that processes features and incorporates time embeddings.\n",
        "   - Contains normalization layers, convolutions, and optional up/downsampling.\n",
        "   - Uses checkpoint function for memory-efficient backpropagation.\n",
        "\n",
        "   4.2. AttentionBlock:\n",
        "   - Self-attention mechanism allowing interaction between different parts of the sequence.\n",
        "   - Uses QKVAttention for efficient attention computation.\n",
        "\n",
        "   4.3. SpatialTransformer:\n",
        "   - More sophisticated attention mechanism with multiple transformer layers.\n",
        "   - Each layer contains self-attention and feed-forward networks.\n",
        "\n",
        "   4.4. CrossAttention:\n",
        "   - Attention mechanism that can attend to a separate context (used in SpatialTransformer).\n",
        "   - Splits input into query, key, and value before computing attention.\n",
        "\n",
        "   4.5. FeedForward:\n",
        "   - Simple feedforward network used in transformer blocks.\n",
        "   - Contains two linear layers with GELU activation and dropout.\n",
        "\n",
        "   4.6. Upsample and Downsample:\n",
        "   - Handle changes in spatial dimensions of feature maps.\n",
        "   - Use either interpolation or transposed convolutions.\n",
        "\n",
        "   4.7. GroupNorm32:\n",
        "   - Custom group normalization for improved training stability.\n",
        "\n",
        "   4.8. TimestepEmbedSequential:\n",
        "   - Sequential module that handles passing of timestep embeddings to appropriate submodules.\n",
        "\n",
        "5. FlowMatchingTrainer:\n",
        "   - Manages the training process of the UNetModel.\n",
        "   - Implements the forward process (adding noise) and reverse process (denoising).\n",
        "   - Uses a noise schedule to control the amount of noise added at each timestep.\n",
        "   - Computes loss based on the model's ability to predict the noise added.\n",
        "\n",
        "6. ProtT5Encoder (for training):\n",
        "   - Utilizes a pre-trained ProtT5 model to encode protein sequences into a latent space.\n",
        "   - Processes amino acid sequences into a high-dimensional representation.\n",
        "\n",
        "7. ProtT5Decoder (for inference):\n",
        "   - Converts latent representations back into amino acid sequences.\n",
        "   - Uses beam search or other decoding strategies to generate the final protein sequence.\n",
        "\n",
        "8. Training Process:\n",
        "   8.1. Data Preparation:\n",
        "   - Batch of scVI embeddings and corresponding protein sequences are loaded.\n",
        "   - Protein sequences are encoded using ProtT5Encoder.\n",
        "\n",
        "   8.2. Forward Pass:\n",
        "   - Random timesteps are generated for each sample in the batch.\n",
        "   - Noise is added to the encoded protein sequences based on the timesteps.\n",
        "   - The UNetModel processes the noisy encodings, conditioned on scVI embeddings and timesteps.\n",
        "\n",
        "   8.3. Loss Computation:\n",
        "   - The model's output is compared to the true noise added.\n",
        "   - Loss is calculated (usually mean squared error).\n",
        "\n",
        "   8.4. Backpropagation:\n",
        "   - Gradients are computed and model parameters are updated.\n",
        "\n",
        "9. Inference Process:\n",
        "   9.1. Start with an scVI embedding of a cell state.\n",
        "   9.2. Generate random noise as the starting point.\n",
        "   9.3. Gradually denoise using the UNetModel:\n",
        "      - For each timestep (from most noisy to least):\n",
        "        - Pass the current noisy sample through the UNetModel.\n",
        "        - Use the model's prediction to update the sample.\n",
        "   9.4. The final denoised representation is passed through the ProtT5Decoder.\n",
        "   9.5. The decoder outputs the generated protein sequence.\n",
        "\n",
        "10. Utility Functions:\n",
        "    - conv_nd: Creates 1D convolutions for our sequence data.\n",
        "    - zero_module: Initializes a module's parameters to zero.\n",
        "    - normalization: Applies GroupNorm32 normalization.\n",
        "    - checkpoint: Implements gradient checkpointing for memory efficiency.\n",
        "    - exists and default: Helper functions for handling optional parameters.\n",
        "\n",
        "Cool adaptations\n",
        "    - Adaptation of 2D UNet architecture to 1D protein sequences.\n",
        "        - Unet in original model was used for the audio representations (spectograms)\n",
        "        - While we do use protein embeddings (like those from ProtT5), the UNet in our case still operates on a 1D sequence of these embeddings.\n",
        "        - Each position in this sequence corresponds to an amino acid, but is represented by a high-dimensional vector.\n",
        "        - The UNet processes this sequence of vectors, maintaining the 1D structure of the protein\n",
        "            - each element of the sequence is itself a rich high-dimensional representation (1280)\n",
        "    - Integration of flow matching with protein language models.\n",
        "    - Use of scVI embeddings as conditional input for targeted protein generation.\n",
        "\n",
        "\n",
        "Loss:\n",
        "θ^ = argmin_θ E_t,z_t ||u_θ(z_t, t, c) - v_t||^2\n",
        "Where:\n",
        "\n",
        "u_θ is your flow matching model (UNet)\n",
        "z_t is the scVI embedding at time t\n",
        "t is the timestep\n",
        "c is your context (which we'll discuss next)\n",
        "v_t is the target velocity (z_1 - (1-σ_min)z_0 in the optimal transport formulation)\n",
        "\n",
        "\n",
        "UNet\n",
        "This 1D UNet processes the scVI latent representations, which encode cellular states, through a series of downsampling and upsampling operations.\n",
        "The input blocks progressively reduce the spatial dimensions while increasing the channel depth, capturing hierarchical features.\n",
        "The middle block, with its deep channel representation, allows for global reasoning across the entire sequence.\n",
        "The output blocks then gradually upsample the representation back to the original dimensions, utilizing skip connections to preserve fine-grained information.\n",
        "Time embeddings are crucial, allowing the model to understand its position in the generation process.\n",
        "These embeddings are added to the input at each step, guiding the transformation from noise to protein sequence.\n",
        "Attention mechanisms, implemented either as AttentionBlocks or SpatialTransformers, enable the model to capture long-range dependencies critical for protein structure.\n",
        "The ResBlocks incorporate both the current state and the time embedding, allowing for time-dependent processing at each level.\n",
        "The context dimension, which could include additional information like pseudotime or motif data, is integrated through the SpatialTransformer blocks, providing extra conditioning for the generation process.\n",
        "The model's output represents the velocity field in the flow matching framework, predicting how the latent representation should change at each step to transform noise into a meaningful protein sequence representation.\n",
        "\n"
      ],
      "metadata": {
        "id": "vGFeB3nUkW7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## library installs"
      ],
      "metadata": {
        "id": "b-njBgLzltjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch anndata scvi-tools einops numpy scipy transformers scanpy"
      ],
      "metadata": {
        "id": "P6I50Tquluub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fab7261-36e9-4e09-c3f5-99f4e5e7a314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: anndata in /usr/local/lib/python3.10/dist-packages (0.10.8)\n",
            "Requirement already satisfied: scvi-tools in /usr/local/lib/python3.10/dist-packages (1.1.5)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: scanpy in /usr/local/lib/python3.10/dist-packages (1.10.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
            "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata) (1.2.2)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from anndata) (3.11.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from anndata) (8.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from anndata) (24.1)\n",
            "Requirement already satisfied: pandas!=2.1.0rc0,!=2.1.2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from anndata) (2.1.4)\n",
            "Requirement already satisfied: docrep>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.3.2)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.8.4)\n",
            "Requirement already satisfied: jax>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.4.26)\n",
            "Requirement already satisfied: jaxlib>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: lightning<2.2,>=2.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (2.1.4)\n",
            "Requirement already satisfied: ml-collections>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.1.1)\n",
            "Requirement already satisfied: mudata>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.3.0)\n",
            "Requirement already satisfied: numpyro>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.15.2)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.2.2)\n",
            "Requirement already satisfied: pyro-ppl>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (1.9.1)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (13.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (1.3.2)\n",
            "Requirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.56.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (4.66.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.4.2)\n",
            "Requirement already satisfied: legacy-api-wrap>=1.4 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.4)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.7.1)\n",
            "Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.5.6)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.5.13)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.13.1)\n",
            "Requirement already satisfied: session-info in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.0.0)\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.14.2)\n",
            "Requirement already satisfied: umap-learn!=0.5.0,>=0.5 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.5.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from docrep>=0.3.2->scvi-tools) (1.16.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.4->scvi-tools) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.4->scvi-tools) (3.3.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning<2.2,>=2.0->scvi-tools) (0.11.6)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning<2.2,>=2.0->scvi-tools) (2.3.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml-collections>=0.1.1->scvi-tools) (1.4.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml-collections>=0.1.1->scvi-tools) (21.6.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.56->scanpy) (0.43.0)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from numpyro>=0.12.1->scvi-tools) (1.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.1)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl>=1.6.0->scvi-tools) (0.1.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->scvi-tools) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->scvi-tools) (2.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.2->scvi-tools) (3.5.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax->scvi-tools) (1.0.8)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax->scvi-tools) (0.5.23)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax->scvi-tools) (0.1.63)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax->scvi-tools) (0.1.86)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: stdlib-list in /usr/local/lib/python3.10/dist-packages (from session-info->scanpy) (0.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax->scvi-tools) (0.12.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning<2.2,>=2.0->scvi-tools) (71.0.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->scvi-tools) (0.1.2)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->scvi-tools) (1.7.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->scvi-tools) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->scvi-tools) (3.20.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (4.0.3)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->scvi-tools) (6.4.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->scvi-tools) (3.19.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils"
      ],
      "metadata": {
        "id": "X-e-EUQyklC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) 2023 Amphion.\n",
        "#\n",
        "# This source code is licensed under the MIT license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class AbstractDistribution:\n",
        "    def sample(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def mode(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "class DiracDistribution(AbstractDistribution):\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "\n",
        "    def sample(self):\n",
        "        return self.value\n",
        "\n",
        "    def mode(self):\n",
        "        return self.value\n",
        "\n",
        "\n",
        "class DiagonalGaussianDistribution(object):\n",
        "    def __init__(self, parameters, deterministic=False):\n",
        "        self.parameters = parameters\n",
        "        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n",
        "        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n",
        "        self.deterministic = deterministic\n",
        "        self.std = torch.exp(0.5 * self.logvar)\n",
        "        self.var = torch.exp(self.logvar)\n",
        "        if self.deterministic:\n",
        "            self.var = self.std = torch.zeros_like(self.mean).to(\n",
        "                device=self.parameters.device\n",
        "            )\n",
        "\n",
        "    def sample(self):\n",
        "        x = self.mean + self.std * torch.randn(self.mean.shape).to(\n",
        "            device=self.parameters.device\n",
        "        )\n",
        "        return x\n",
        "\n",
        "    def kl(self, other=None):\n",
        "        if self.deterministic:\n",
        "            return torch.Tensor([0.0])\n",
        "        else:\n",
        "            if other is None:\n",
        "                return 0.5 * torch.sum(\n",
        "                    torch.pow(self.mean, 2) + self.var - 1.0 - self.logvar,\n",
        "                    dim=[1, 2, 3],\n",
        "                )\n",
        "            else:\n",
        "                return 0.5 * torch.sum(\n",
        "                    torch.pow(self.mean - other.mean, 2) / other.var\n",
        "                    + self.var / other.var\n",
        "                    - 1.0\n",
        "                    - self.logvar\n",
        "                    + other.logvar,\n",
        "                    dim=[1, 2, 3],\n",
        "                )\n",
        "\n",
        "    def nll(self, sample, dims=[1, 2, 3]):\n",
        "        if self.deterministic:\n",
        "            return torch.Tensor([0.0])\n",
        "        logtwopi = np.log(2.0 * np.pi)\n",
        "        return 0.5 * torch.sum(\n",
        "            logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var,\n",
        "            dim=dims,\n",
        "        )\n",
        "\n",
        "    def mode(self):\n",
        "        return self.mean\n",
        "\n",
        "\n",
        "def normal_kl(mean1, logvar1, mean2, logvar2):\n",
        "    \"\"\"\n",
        "    source: https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/losses.py#L12\n",
        "    Compute the KL divergence between two gaussians.\n",
        "    Shapes are automatically broadcasted, so batches can be compared to\n",
        "    scalars, among other use cases.\n",
        "    \"\"\"\n",
        "    tensor = None\n",
        "    for obj in (mean1, logvar1, mean2, logvar2):\n",
        "        if isinstance(obj, torch.Tensor):\n",
        "            tensor = obj\n",
        "            break\n",
        "    assert tensor is not None, \"at least one argument must be a Tensor\"\n",
        "\n",
        "    # Force variances to be Tensors. Broadcasting helps convert scalars to\n",
        "    # Tensors, but it does not work for torch.exp().\n",
        "    logvar1, logvar2 = [\n",
        "        x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor)\n",
        "        for x in (logvar1, logvar2)\n",
        "    ]\n",
        "\n",
        "    return 0.5 * (\n",
        "        -1.0\n",
        "        + logvar2\n",
        "        - logvar1\n",
        "        + torch.exp(logvar1 - logvar2)\n",
        "        + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "g37xH4c6kO8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractmethod\n",
        "from functools import partial\n",
        "import math\n",
        "from typing import Iterable\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from einops import repeat\n",
        "def Normalize(in_channels):\n",
        "    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "def count_flops_attn(model, _x, y):\n",
        "    b, c, *spatial = y[0].shape\n",
        "    num_spatial = int(np.prod(spatial))\n",
        "    matmul_ops = 2 * b * (num_spatial**2) * c\n",
        "    model.total_ops += torch.DoubleTensor([matmul_ops])\n",
        "\n",
        "def timestep_embedding(timesteps, dim, max_period=10000):\n",
        "    half = dim // 2\n",
        "    freqs = torch.exp(\n",
        "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
        "    ).to(device=timesteps.device)\n",
        "    args = timesteps[:, None].float() * freqs[None]\n",
        "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "    if dim % 2:\n",
        "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "    return embedding\n",
        "\n",
        "def conv_nd(dims, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Create a 1D, 2D, or 3D convolution module.\n",
        "    \"\"\"\n",
        "    if dims == 1:\n",
        "        return nn.Conv1d(*args, **kwargs)\n",
        "    elif dims == 2:\n",
        "        return nn.Conv2d(*args, **kwargs)\n",
        "    elif dims == 3:\n",
        "        return nn.Conv3d(*args, **kwargs)\n",
        "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
        "\n",
        "def linear(*args, **kwargs):\n",
        "    \"\"\"\n",
        "    Create a linear module.\n",
        "    \"\"\"\n",
        "    return nn.Linear(*args, **kwargs)\n",
        "\n",
        "def avg_pool_nd(dims, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Create a 1D, 2D, or 3D average pooling module.\n",
        "    \"\"\"\n",
        "    if dims == 1:\n",
        "        return nn.AvgPool1d(*args, **kwargs)\n",
        "    elif dims == 2:\n",
        "        return nn.AvgPool2d(*args, **kwargs)\n",
        "    elif dims == 3:\n",
        "        return nn.AvgPool3d(*args, **kwargs)\n",
        "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"\n",
        "    Zero out the parameters of a module and return it.\n",
        "    \"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "def normalization(channels):\n",
        "    \"\"\"\n",
        "    Make a standard normalization layer.\n",
        "    :param channels: number of input channels.\n",
        "    :return: an nn.Module for normalization.\n",
        "    \"\"\"\n",
        "    return GroupNorm32(32, channels)\n",
        "\n",
        "class GroupNorm32(nn.GroupNorm):\n",
        "    def forward(self, x):\n",
        "        return super().forward(x.float()).type(x.dtype)\n",
        "\n",
        "def checkpoint(func, inputs, params, flag):\n",
        "    \"\"\"\n",
        "    Evaluate a function without caching intermediate activations, allowing for\n",
        "    reduced memory at the expense of extra compute in the backward pass.\n",
        "    \"\"\"\n",
        "    if flag:\n",
        "        args = tuple(inputs) + tuple(params)\n",
        "        return CheckpointFunction.apply(func, len(inputs), *args)\n",
        "    else:\n",
        "        return func(*inputs)\n",
        "\n",
        "class TimestepBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Any module where forward() takes timestep embeddings as a second argument.\n",
        "    \"\"\"\n",
        "    @abstractmethod\n",
        "    def forward(self, x, emb):\n",
        "        \"\"\"\n",
        "        Apply the module to `x` given `emb` timestep embeddings.\n",
        "        \"\"\"\n",
        "\n",
        "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
        "    \"\"\"\n",
        "    A sequential module that passes timestep embeddings to the children that\n",
        "    support it as an extra input.\n",
        "    \"\"\"\n",
        "    def forward(self, x, emb, context=None):\n",
        "        for layer in self:\n",
        "            if isinstance(layer, TimestepBlock):\n",
        "                x = layer(x, emb)\n",
        "            elif isinstance(layer, SpatialTransformer):\n",
        "                x = layer(x, context)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "nAl6JpQjkO-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = default(context_dim, query_dim)\n",
        "\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
        "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, query_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, context=None, mask=None):\n",
        "        h = self.heads\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        context = default(context, x)\n",
        "        k = self.to_k(context)\n",
        "        v = self.to_v(context)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
        "\n",
        "        sim = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = rearrange(mask, 'b ... -> b (...)')\n",
        "            max_neg_value = -torch.finfo(sim.dtype).max\n",
        "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
        "            sim.masked_fill_(~mask, max_neg_value)\n",
        "\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum('b i j, b j d -> b i d', attn, v)\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.0):\n",
        "        super().__init__()\n",
        "        inner_dim = int(dim * mult)\n",
        "        dim_out = default(dim_out, dim)\n",
        "        project_in = nn.Sequential(\n",
        "            nn.Linear(dim, inner_dim),\n",
        "            nn.GELU()\n",
        "        ) if not glu else GEGLU(dim, inner_dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            project_in,\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(inner_dim, dim_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if callable(d) else d\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
        "        return x * F.gelu(gate)\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, channels, use_conv, dims=1, out_channels=None, padding=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.dims = dims\n",
        "        if use_conv:\n",
        "            self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        if self.dims == 3:\n",
        "            x = F.interpolate(x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\")\n",
        "        else:\n",
        "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if self.use_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, channels, use_conv, dims=1, out_channels=None, padding=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.dims = dims\n",
        "        stride = 2 if dims != 3 else (1, 2, 2)\n",
        "        if use_conv:\n",
        "            self.op = conv_nd(dims, self.channels, self.out_channels, 3, stride=stride, padding=padding)\n",
        "        else:\n",
        "            assert self.channels == self.out_channels\n",
        "            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        return self.op(x)\n",
        "\n",
        "class ResBlock(TimestepBlock):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        emb_channels,\n",
        "        dropout,\n",
        "        out_channels=None,\n",
        "        use_conv=False,\n",
        "        use_scale_shift_norm=False,\n",
        "        dims=1,\n",
        "        use_checkpoint=False,\n",
        "        up=False,\n",
        "        down=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.emb_channels = emb_channels\n",
        "        self.dropout = dropout\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.use_scale_shift_norm = use_scale_shift_norm\n",
        "\n",
        "        self.in_layers = nn.Sequential(\n",
        "            normalization(channels),\n",
        "            nn.SiLU(),\n",
        "            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.updown = up or down\n",
        "\n",
        "        if up:\n",
        "            self.h_upd = Upsample(channels, False, dims)\n",
        "            self.x_upd = Upsample(channels, False, dims)\n",
        "        elif down:\n",
        "            self.h_upd = Downsample(channels, False, dims)\n",
        "            self.x_upd = Downsample(channels, False, dims)\n",
        "        else:\n",
        "            self.h_upd = self.x_upd = nn.Identity()\n",
        "\n",
        "        self.emb_layers = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            linear(\n",
        "                emb_channels,\n",
        "                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n",
        "            ),\n",
        "        )\n",
        "        self.out_layers = nn.Sequential(\n",
        "            normalization(self.out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            zero_module(conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)),\n",
        "        )\n",
        "\n",
        "        if self.out_channels == channels:\n",
        "            self.skip_connection = nn.Identity()\n",
        "        elif use_conv:\n",
        "            self.skip_connection = conv_nd(dims, channels, self.out_channels, 3, padding=1)\n",
        "        else:\n",
        "            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n",
        "\n",
        "    def forward(self, x, emb):\n",
        "        return checkpoint(self._forward, (x, emb), self.parameters(), self.use_checkpoint)\n",
        "\n",
        "    def _forward(self, x, emb):\n",
        "        if self.updown:\n",
        "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
        "            h = in_rest(x)\n",
        "            h = self.h_upd(h)\n",
        "            x = self.x_upd(x)\n",
        "            h = in_conv(h)\n",
        "        else:\n",
        "            h = self.in_layers(x)\n",
        "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
        "        while len(emb_out.shape) < len(h.shape):\n",
        "            emb_out = emb_out[..., None]\n",
        "        if self.use_scale_shift_norm:\n",
        "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
        "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
        "            h = out_norm(h) * (1 + scale) + shift\n",
        "            h = out_rest(h)\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "        return self.skip_connection(x) + h\n",
        "\n",
        "class QKVAttention(nn.Module):\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\n",
        "            \"bct,bcs->bts\",\n",
        "            (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),\n",
        "        )\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        num_heads=1,\n",
        "        num_head_channels=-1,\n",
        "        use_checkpoint=False,\n",
        "        use_new_attention_order=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        if num_head_channels == -1:\n",
        "            self.num_heads = num_heads\n",
        "        else:\n",
        "            assert channels % num_head_channels == 0\n",
        "            self.num_heads = channels // num_head_channels\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.norm = normalization(channels)\n",
        "        self.qkv = conv_nd(1, channels, channels * 3, 1)\n",
        "        self.attention = QKVAttention(self.num_heads)\n",
        "        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return checkpoint(self._forward, (x,), self.parameters(), self.use_checkpoint)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        b, c, *spatial = x.shape\n",
        "        x = x.reshape(b, c, -1)\n",
        "        qkv = self.qkv(self.norm(x))\n",
        "        h = self.attention(qkv)\n",
        "        h = self.proj_out(h)\n",
        "        return (x + h).reshape(b, c, *spatial)\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self, in_channels, n_heads, d_head, depth=1, dropout=0., context_dim=None):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        inner_dim = n_heads * d_head\n",
        "        self.norm = Normalize(in_channels)\n",
        "\n",
        "        self.proj_in = nn.Conv1d(in_channels, inner_dim, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [BasicTransformerBlock(inner_dim, n_heads, d_head, dropout=dropout, context_dim=context_dim)\n",
        "             for d in range(depth)]\n",
        "        )\n",
        "\n",
        "        self.proj_out = zero_module(nn.Conv1d(inner_dim, in_channels, kernel_size=1, stride=1, padding=0))\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        # note: if no context is given, cross-attention defaults to self-attention\n",
        "        b, c, s = x.shape\n",
        "        x_in = x\n",
        "        x = self.norm(x)\n",
        "        x = self.proj_in(x)\n",
        "        x = rearrange(x, 'b c s -> b s c')\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, context=context)\n",
        "        x = rearrange(x, 'b s c -> b c s')\n",
        "        x = self.proj_out(x)\n",
        "        return x + x_in\n",
        "\n",
        "class BasicTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, n_heads, d_head, dropout=0., context_dim=None, gated_ff=True, checkpoint=True):\n",
        "        super().__init__()\n",
        "        self.attn1 = CrossAttention(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout)\n",
        "        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)\n",
        "        self.attn2 = CrossAttention(query_dim=dim, context_dim=context_dim,\n",
        "                                    heads=n_heads, dim_head=d_head, dropout=dropout)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "        self.checkpoint = checkpoint\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        return checkpoint(self._forward, (x, context), self.parameters(), self.checkpoint)\n",
        "\n",
        "    def _forward(self, x, context=None):\n",
        "        x = self.attn1(self.norm1(x)) + x\n",
        "        x = self.attn2(self.norm2(x), context=context) + x\n",
        "        x = self.ff(self.norm3(x)) + x\n",
        "        return x"
      ],
      "metadata": {
        "id": "w0P4txPYkPAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pretrained embedding modules (prott5, scvi)"
      ],
      "metadata": {
        "id": "ht7z5O8ilEnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5EncoderModel, T5Tokenizer\n",
        "import torch.nn as nn\n",
        "import re\n",
        "\n",
        "class ProtT5EncodingModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.protT5_model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
        "        self.protT5_tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        processed_seq = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))\n",
        "        ids = self.protT5_tokenizer(processed_seq, add_special_tokens=True, return_tensors=\"pt\", padding='longest')\n",
        "        input_ids = ids['input_ids'].to(self.protT5_model.device)\n",
        "        attention_mask = ids['attention_mask'].to(self.protT5_model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding_repr = self.protT5_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        seq_emb = embedding_repr.last_hidden_state\n",
        "        return seq_emb"
      ],
      "metadata": {
        "id": "AQRVEnwZlULP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "class ProtT5DecodingModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.protT5_model = T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "        self.protT5_tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "\n",
        "    def forward(self, latent_repr, max_length=200):\n",
        "        outputs = self.protT5_model.generate(\n",
        "            inputs_embeds=latent_repr,\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        decoded_sequences = self.protT5_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        return decoded_sequences"
      ],
      "metadata": {
        "id": "72smUTRJkPCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scvi\n",
        "import scanpy as sc\n",
        "import numpy as np\n",
        "\n",
        "# encode pseudotime and latent representations\n",
        "\n",
        "class SCVIEncodingModule:\n",
        "    def __init__(self):\n",
        "        self.latent_representations = {}\n",
        "        self.pseudotime_representations = {}\n",
        "\n",
        "    def encode(self, adata_dict):\n",
        "        for cell_type, adata in adata_dict.items():\n",
        "            print(f\"Training and embedding for cell type: {cell_type}...\")\n",
        "\n",
        "            adata_copy = adata.copy()\n",
        "\n",
        "            nan_count = np.isnan(adata_copy.X).sum()\n",
        "            if nan_count > 0:\n",
        "                print(f\"There are {nan_count} NaN values in adata_copy.X for cell type: {cell_type}\")\n",
        "            else:\n",
        "                print(f\"No NaN values found in adata_copy.X for cell type: {cell_type}\")\n",
        "\n",
        "            latent = adata_copy.obsm['X_scvi']\n",
        "            pseudotime = adata_copy.obs['dpt_pseudotime']\n",
        "\n",
        "            # Store latent representation in the dictionary\n",
        "            self.latent_representations[cell_type] = latent\n",
        "            self.pseudotime_representations[cell_type] = pseudotime\n",
        "\n",
        "        print(\"Encoding completed.\")\n",
        "\n",
        "        return self.latent_representations, self.pseudotime_representations"
      ],
      "metadata": {
        "id": "OeMMOtkGuWQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## unet model (flow matching)"
      ],
      "metadata": {
        "id": "PBEPeCXzk3zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomUNet1D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        model_channels,\n",
        "        num_res_blocks,\n",
        "        attention_resolutions,\n",
        "        dropout=0,\n",
        "        channel_mult=(1, 2, 4, 8),\n",
        "        conv_resample=True,\n",
        "        use_checkpoint=False,\n",
        "        num_heads=8,\n",
        "        use_scale_shift_norm=False,\n",
        "        use_spatial_transformer=False,\n",
        "        transformer_depth=1,\n",
        "        context_dim=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attention_resolutions = attention_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.channel_mult = channel_mult\n",
        "        self.conv_resample = conv_resample\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.num_heads = num_heads\n",
        "        self.use_spatial_transformer = use_spatial_transformer\n",
        "        self.context_dim = context_dim\n",
        "\n",
        "        self.context_proj = nn.Linear(context_dim, model_channels)\n",
        "        self.final_proj = nn.Linear(model_channels * 2, out_channels)  # *2 to account for concatenated context\n",
        "\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(model_channels, time_embed_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim),\n",
        "        )\n",
        "\n",
        "        self.input_blocks = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels, model_channels, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        ch = model_channels\n",
        "        input_block_chans = [model_channels]\n",
        "        ds = 1\n",
        "        for level, mult in enumerate(channel_mult):\n",
        "            for _ in range(num_res_blocks):\n",
        "                layers = [\n",
        "                    ResBlock1D(\n",
        "                        ch,\n",
        "                        time_embed_dim,\n",
        "                        dropout,\n",
        "                        out_channels=mult * model_channels,\n",
        "                        use_scale_shift_norm=use_scale_shift_norm,\n",
        "                    )\n",
        "                ]\n",
        "                ch = mult * model_channels\n",
        "                if ds in attention_resolutions:\n",
        "                    if use_spatial_transformer:\n",
        "                        layers.append(\n",
        "                            SpatialTransformer1D(\n",
        "                                ch, num_heads, context_dim, depth=transformer_depth\n",
        "                            )\n",
        "                        )\n",
        "                    else:\n",
        "                        layers.append(AttentionBlock1D(ch, num_heads=num_heads))\n",
        "                self.input_blocks.append(nn.Sequential(*layers))\n",
        "                input_block_chans.append(ch)\n",
        "            if level != len(channel_mult) - 1:\n",
        "                self.input_blocks.append(\n",
        "                    nn.Conv1d(ch, ch, 3, stride=2, padding=1)\n",
        "                )\n",
        "                input_block_chans.append(ch)\n",
        "                ds *= 2\n",
        "\n",
        "        self.middle_block = nn.Sequential(\n",
        "            ResBlock1D(\n",
        "                ch,\n",
        "                time_embed_dim,\n",
        "                dropout,\n",
        "                use_scale_shift_norm=use_scale_shift_norm,\n",
        "            ),\n",
        "            AttentionBlock1D(ch, num_heads=num_heads) if not use_spatial_transformer else\n",
        "            SpatialTransformer1D(ch, num_heads, context_dim, depth=transformer_depth),\n",
        "            ResBlock1D(\n",
        "                ch,\n",
        "                time_embed_dim,\n",
        "                dropout,\n",
        "                use_scale_shift_norm=use_scale_shift_norm,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.output_blocks = nn.ModuleList([])\n",
        "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
        "            for i in range(num_res_blocks + 1):\n",
        "                layers = [\n",
        "                    ResBlock1D(\n",
        "                        ch + input_block_chans.pop(),\n",
        "                        time_embed_dim,\n",
        "                        dropout,\n",
        "                        out_channels=model_channels * mult,\n",
        "                        use_scale_shift_norm=use_scale_shift_norm,\n",
        "                    )\n",
        "                ]\n",
        "                ch = model_channels * mult\n",
        "                if ds in attention_resolutions:\n",
        "                    if use_spatial_transformer:\n",
        "                        layers.append(\n",
        "                            SpatialTransformer1D(\n",
        "                                ch, num_heads, context_dim, depth=transformer_depth\n",
        "                            )\n",
        "                        )\n",
        "                    else:\n",
        "                        layers.append(AttentionBlock1D(ch, num_heads=num_heads))\n",
        "                if level and i == num_res_blocks:\n",
        "                    layers.append(nn.ConvTranspose1d(ch, ch, 4, stride=2, padding=1))\n",
        "                    ds //= 2\n",
        "                self.output_blocks.append(nn.Sequential(*layers))\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(32, ch),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv1d(ch, out_channels, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, timesteps, context=None):\n",
        "      # x: [batch_size, 1024, 50]\n",
        "      # context: [batch_size, seq_len, context_dim]\n",
        "\n",
        "      x = x.transpose(1, 2)  # transpose shape: [batch_size, emb_dim, seqlen]\n",
        "      t_emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
        "\n",
        "      h = x\n",
        "      hs = []\n",
        "      for module in self.input_blocks:\n",
        "          # print(\"Module type input blocks:\", type(module))\n",
        "          if isinstance(module, nn.Conv1d):\n",
        "              h = module(h)\n",
        "          elif isinstance(module, SpatialTransformer1D):\n",
        "            h = module(h, context)\n",
        "          elif isinstance(module, ResBlock1D):\n",
        "            h = module(h, t_emb)\n",
        "          elif isinstance(module, nn.Sequential):\n",
        "              for submodule in module:\n",
        "                  # print(\"submodule type input blocks:\", type(submodule))\n",
        "                  if isinstance(submodule, ResBlock1D):\n",
        "                      h = submodule(h, t_emb)\n",
        "                  elif isinstance(submodule, SpatialTransformer1D):\n",
        "                      h = submodule(h, context)\n",
        "                  else:\n",
        "                      h = submodule(h)\n",
        "          else:\n",
        "              h = module(h)\n",
        "          hs.append(h)\n",
        "\n",
        "      if isinstance(self.middle_block, nn.Sequential):\n",
        "          for submodule in self.middle_block:\n",
        "              if isinstance(submodule, ResBlock1D):\n",
        "                  h = submodule(h, t_emb)\n",
        "              elif isinstance(submodule, SpatialTransformer1D):\n",
        "                  h = submodule(h, context)\n",
        "              else:\n",
        "                  h = submodule(h)\n",
        "      else:\n",
        "          h = self.middle_block(h)\n",
        "\n",
        "      for module in self.output_blocks:\n",
        "          h = torch.cat([h, hs.pop()], dim=1)\n",
        "          if isinstance(module, nn.Sequential):\n",
        "              for submodule in module:\n",
        "                  if isinstance(submodule, ResBlock1D):\n",
        "                      h = submodule(h, t_emb)\n",
        "                  elif isinstance(submodule, SpatialTransformer1D):\n",
        "                      h = submodule(h, context)\n",
        "                  else:\n",
        "                      h = submodule(h)\n",
        "          elif isinstance(module, SpatialTransformer1D):\n",
        "            h = module(h, context)\n",
        "          elif isinstance(module, ResBlock1D):\n",
        "            h = module(h, t_emb)\n",
        "          else:\n",
        "              h = module(h)\n",
        "\n",
        "      output = self.out(h) # [batch_size, model_channels, seq_len]\n",
        "      output = output.transpose(1, 2)  # shape: [batch_size, seqlen, model_channels]\n",
        "\n",
        "      return output\n",
        "\n",
        "class ResBlock1D(nn.Module):\n",
        "    def __init__(self, channels, time_embed_dim, dropout, out_channels=None, use_scale_shift_norm=False):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.time_embed_dim = time_embed_dim\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_scale_shift_norm = use_scale_shift_norm\n",
        "\n",
        "        self.in_layers = nn.Sequential(\n",
        "            nn.GroupNorm(32, channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv1d(channels, self.out_channels, 3, padding=1),\n",
        "        )\n",
        "        self.emb_layers = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, 2 * self.out_channels if use_scale_shift_norm else self.out_channels),\n",
        "        )\n",
        "        self.out_layers = nn.Sequential(\n",
        "            nn.GroupNorm(32, self.out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv1d(self.out_channels, self.out_channels, 3, padding=1),\n",
        "        )\n",
        "        if channels != self.out_channels:\n",
        "            self.skip_connection = nn.Conv1d(channels, self.out_channels, 1)\n",
        "        else:\n",
        "            self.skip_connection = nn.Identity()\n",
        "\n",
        "    def forward(self, x, emb):\n",
        "        h = self.in_layers(x)\n",
        "        emb_out = self.emb_layers(emb).unsqueeze(2)\n",
        "        if self.use_scale_shift_norm:\n",
        "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
        "            h = self.out_layers[0](h) * (1 + scale) + shift\n",
        "            h = self.out_layers[1:](h)\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "        return self.skip_connection(x) + h\n",
        "\n",
        "class AttentionBlock1D(nn.Module):\n",
        "    def __init__(self, channels, num_heads=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.norm = nn.GroupNorm(32, channels)\n",
        "        self.qkv = nn.Conv1d(channels, channels * 3, 1)\n",
        "        self.attention = QKVAttention(num_heads)\n",
        "        self.proj_out = nn.Conv1d(channels, channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, s = x.shape\n",
        "        qkv = self.qkv(self.norm(x))\n",
        "        qkv = qkv.reshape(b * self.num_heads, -1, s)\n",
        "        h = self.attention(qkv)\n",
        "        h = h.reshape(b, -1, s)\n",
        "        return x + self.proj_out(h)\n",
        "\n",
        "class QKVAttention(nn.Module):\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        bs, width, length = qkv.shape\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\n",
        "            \"bct,bcs->bts\",\n",
        "            (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),\n",
        "        )\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "class SpatialTransformer1D(nn.Module):\n",
        "    def __init__(self, channels, num_heads, context_dim, depth=1):\n",
        "        super().__init__()\n",
        "        self.norm = nn.GroupNorm(32, channels)\n",
        "        inner_dim = channels\n",
        "        self.proj_in = nn.Conv1d(channels, inner_dim, 1)\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [BasicTransformerBlock(inner_dim, num_heads, context_dim) for _ in range(depth)]\n",
        "        )\n",
        "        self.proj_out = nn.Conv1d(inner_dim, channels, 1)\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        b, c, s = x.shape\n",
        "        x_in = x\n",
        "        x = self.norm(x)\n",
        "        x = self.proj_in(x)\n",
        "        x = x.permute(0, 2, 1).contiguous()\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, context)\n",
        "        x = x.permute(0, 2, 1).contiguous()\n",
        "        x = self.proj_out(x)\n",
        "        return x + x_in\n",
        "\n",
        "class BasicTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, context_dim):\n",
        "        super().__init__()\n",
        "        self.attn1 = CrossAttention(dim, dim, num_heads)\n",
        "        self.ff = FeedForward(dim)\n",
        "        self.attn2 = CrossAttention(dim, context_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        x = self.attn1(self.norm1(x)) + x\n",
        "        x = self.attn2(self.norm2(x), context=context) + x\n",
        "        x = self.ff(self.norm3(x)) + x\n",
        "        return x\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim, num_heads, dim_head=64):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * num_heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
        "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        h = self.num_heads\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        context = x if context is None else context\n",
        "        k = self.to_k(context)\n",
        "        v = self.to_v(context)\n",
        "\n",
        "        q, k, v = map(lambda t: t.reshape(t.shape[0], -1, h, t.shape[-1] // h).permute(0, 2, 1, 3), (q, k, v))\n",
        "        sim = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
        "        out = out.permute(0, 2, 1, 3).reshape(out.shape[0], -1, out.shape[-1] * h)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim * mult, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def timestep_embedding(timesteps, dim, max_period=10000):\n",
        "    half = dim // 2\n",
        "    freqs = torch.exp(\n",
        "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
        "    ).to(device=timesteps.device)\n",
        "    args = timesteps[:, None].float() * freqs[None]\n",
        "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "    if dim % 2:\n",
        "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "    return embedding\n",
        "\n",
        "class TimestepEmbedSequential(nn.Sequential):\n",
        "    def forward(self, x, emb, context=None):\n",
        "        for layer in self:\n",
        "            if isinstance(layer, ResBlock1D):\n",
        "                x = layer(x, emb)\n",
        "            elif isinstance(layer, SpatialTransformer1D):\n",
        "                x = layer(x, context)\n",
        "            elif isinstance(layer, nn.Conv1d) or isinstance(layer, nn.GroupNorm) or isinstance(layer, nn.ReLU):\n",
        "                x = layer(x)\n",
        "            else:\n",
        "                x = layer(x, emb, context)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vtTtnoVYk9B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ode solver"
      ],
      "metadata": {
        "id": "zOjxMpSTljVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.integrate import solve_ivp\n",
        "import numpy as np\n",
        "\n",
        "class ODESolverModule:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def ode_func(self, t, y, *args):\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(0)\n",
        "        t_tensor = torch.tensor([t], dtype=torch.float32)\n",
        "        with torch.no_grad():\n",
        "            dy_dt = self.model(y_tensor, t_tensor, *args).squeeze().numpy()\n",
        "        return dy_dt\n",
        "\n",
        "    def solve(self, y0, t_span, *args, method='RK45', **kwargs):\n",
        "        solution = solve_ivp(\n",
        "            fun=lambda t, y: self.ode_func(t, y, *args),\n",
        "            t_span=t_span,\n",
        "            y0=y0,\n",
        "            method=method,\n",
        "            **kwargs\n",
        "        )\n",
        "        return solution"
      ],
      "metadata": {
        "id": "HN_dc7YsllVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## flow trainer"
      ],
      "metadata": {
        "id": "yQZhv_Qwk3Rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FlowMatchingTrainer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        init_type=\"gaussian\",\n",
        "        noise_scale=1.0,\n",
        "        reflow_t_schedule=\"uniform\",\n",
        "        use_ode_sampler=\"euler\",\n",
        "        sigma_var=0.0,\n",
        "        ode_tol=1e-5,\n",
        "        sample_N=25,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.init_type = init_type\n",
        "        self.noise_scale = noise_scale\n",
        "        self.reflow_t_schedule = reflow_t_schedule\n",
        "        self.use_ode_sampler = use_ode_sampler\n",
        "        self.sigma_var = sigma_var\n",
        "        self.ode_tol = ode_tol\n",
        "        self.sample_N = sample_N\n",
        "        self.T = 1\n",
        "        self.eps = 1e-3\n",
        "        self.sigma_t = lambda t: (1.0 - t) * sigma_var\n",
        "\n",
        "    def forward(self, x_0, prot_target, c):\n",
        "        # x_0: scVI embeddings [batch_size, 1024, 50]\n",
        "        # prot_target: ProtT5 embeddings [batch_size, seq_len, 1024]\n",
        "        # context: pseudotime or other context [batch_size, seq_len, context_dim=1]\n",
        "\n",
        "        # pad prot_target to match x_0's sequence length\n",
        "        pad_length = x_0.shape[1] - prot_target.shape[1]\n",
        "        prot_target_padded = F.pad(prot_target, (0, 0, 0, pad_length))\n",
        "\n",
        "        t = torch.rand(x_0.shape[0], device=x_0.device) * (self.T - self.eps) + self.eps\n",
        "        t_expand = t.view(-1, 1, 1).repeat(1, prot_target_padded.shape[1], prot_target_padded.shape[2])\n",
        "\n",
        "        c = c.to(x_0.device)\n",
        "\n",
        "        noise = torch.randn_like(prot_target_padded)\n",
        "        # print(prot_target_padded.shape)\n",
        "        perturbed_target = t_expand * prot_target_padded + (1 - t_expand) * noise\n",
        "\n",
        "        model_out = self.model(x_0, t * 999, c)\n",
        "\n",
        "        # print(\"___________________________________\")\n",
        "        # print(\"loss function params\")\n",
        "        # print(\"model out shape\", model_out.shape)\n",
        "        # print(\"target shape\", prot_target_padded.shape)\n",
        "\n",
        "        loss = F.mse_loss(model_out, prot_target_padded, reduction=\"none\").mean([1, 2]).mean()\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def euler_sample(self, cond, shape, guidance_scale):\n",
        "        device = self.model.device\n",
        "        batch = torch.randn(shape, device=device)\n",
        "        x = torch.randn_like(batch)\n",
        "        dt = 1.0 / self.sample_N\n",
        "        eps = 1e-3\n",
        "        for i in range(self.sample_N):\n",
        "            num_t = i / self.sample_N * (self.T - eps) + eps\n",
        "            t = torch.ones(batch.shape[0], device=device) * num_t\n",
        "\n",
        "            model_out = self.model(torch.cat([x] * 2), torch.cat([t * 999] * 2), cond)\n",
        "            noise_pred_uncond, noise_pred_text = model_out.chunk(2)\n",
        "            pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            sigma_t = self.sigma_t(num_t)\n",
        "            pred_sigma = pred + (sigma_t**2) / (2 * (self.noise_scale**2) * ((1.0 - num_t) ** 2)) * (\n",
        "                0.5 * num_t * (1.0 - num_t) * pred - 0.5 * (2.0 - num_t) * x.detach().clone()\n",
        "            )\n",
        "\n",
        "            x = x.detach().clone() + pred_sigma * dt + sigma_t * np.sqrt(dt) * torch.randn_like(pred_sigma).to(device)\n",
        "\n",
        "        return x, self.sample_N"
      ],
      "metadata": {
        "id": "2TZyCbPplciH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train module"
      ],
      "metadata": {
        "id": "0W819YWdTzCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### scvi encoding"
      ],
      "metadata": {
        "id": "21dEMxSOw1ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import anndata\n",
        "import scvi\n",
        "import os\n",
        "\n",
        "file_count = 0\n",
        "\n",
        "adata_list = []\n",
        "folder_path = '/content/drive/MyDrive/tf-flow-design/combined_adata_folder/'\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.h5ad'):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        print(filename)\n",
        "        adata = anndata.read_h5ad(file_path)\n",
        "        adata_list.append(adata)\n",
        "        file_count += 1\n",
        "        if file_count >= 5:\n",
        "            break\n",
        "\n",
        "print(f'Read and stored {file_count} .h5ad files.')"
      ],
      "metadata": {
        "id": "_KZ5OfT8kPNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "427da6b4-be9d-47f7-fa0c-c5638ea6771c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combined_adata_macrophage.h5ad\n",
            "combined_adata_monocyte.h5ad\n",
            "combined_adata_endothelial cell of hepatic sinusoid.h5ad\n",
            "combined_adata_liver dendritic cell.h5ad\n",
            "combined_adata_nk cell.h5ad\n",
            "Read and stored 5 .h5ad files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Concatenate all AnnData objects\n",
        "# combined_adata = anndata.concat(adata_list, join='outer', label='batch')\n",
        "# print(f\"Combined AnnData shape: {combined_adata.shape}\")"
      ],
      "metadata": {
        "id": "78QR-aHoi5zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import anndata\n",
        "\n",
        "adata_dict = {}\n",
        "\n",
        "for adata in adata_list:\n",
        "    # Get the unique cell_ontology_class values (excluding 'mesenchymal stem cell')\n",
        "    cell_types = adata.obs['cell_ontology_class'].unique()\n",
        "    for cell_type in cell_types:\n",
        "        if cell_type != 'mesenchymal stem cell':\n",
        "            if cell_type not in adata_dict:\n",
        "                adata_dict[cell_type] = adata\n",
        "            else:\n",
        "                adata_dict[cell_type] = anndata.concat([adata_dict[cell_type], adata])\n"
      ],
      "metadata": {
        "id": "Lts1Qur2av3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adata_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxoZTz7QQaP9",
        "outputId": "db880004-f064-4ba9-d607-ec5aa36caec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['macrophage', 'monocyte', 'endothelial cell of hepatic sinusoid', 'liver dendritic cell', 'nk cell'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scvi_encoder = SCVIEncodingModule()\n",
        "scvi_latents, scvi_pseudotimes = scvi_encoder.encode(adata_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8zBTpfQQ5KW",
        "outputId": "82e0924c-e564-4ef8-dab9-67044dbcbab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and embedding for cell type: macrophage...\n",
            "No NaN values found in adata_copy.X for cell type: macrophage\n",
            "Training and embedding for cell type: monocyte...\n",
            "No NaN values found in adata_copy.X for cell type: monocyte\n",
            "Training and embedding for cell type: endothelial cell of hepatic sinusoid...\n",
            "No NaN values found in adata_copy.X for cell type: endothelial cell of hepatic sinusoid\n",
            "Training and embedding for cell type: liver dendritic cell...\n",
            "No NaN values found in adata_copy.X for cell type: liver dendritic cell\n",
            "Training and embedding for cell type: nk cell...\n",
            "No NaN values found in adata_copy.X for cell type: nk cell\n",
            "Encoding completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scvi_latents['macrophage'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_xhzSQ7v5uU",
        "outputId": "cc282fe1-6abb-447f-9623-ebbb029aca35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50663, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scvi_pseudotimes['macrophage'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37G0qSqw5mve",
        "outputId": "37cb2402-2cfc-4bd4-b5fd-01791212cfad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50663,)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### generate random protein sequences for each scvi latent (3 pos, 10 neg)"
      ],
      "metadata": {
        "id": "JsBKL442wzXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# valid AAs\n",
        "valid_amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
        "\n",
        "def generate_random_protein_sequence(min_length=100, max_length=200):\n",
        "    length = random.randint(min_length, max_length)\n",
        "    return ''.join(random.choices(valid_amino_acids, k=length))\n",
        "\n",
        "protein_sequences = {}\n",
        "\n",
        "for cell_type in scvi_latents.keys():\n",
        "    protein_sequences[cell_type] = [generate_random_protein_sequence() for _ in range(3)]\n",
        "\n",
        "cell_type_example = 'macrophage'\n",
        "print(f\"prot seqs for {cell_type_example}:\")\n",
        "for seq in protein_sequences[cell_type_example]:\n",
        "    print(seq)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG1QHIYTw7lL",
        "outputId": "b4f8155a-d27e-434a-8788-90ac07306ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prot seqs for macrophage:\n",
            "NTQHPLMGGHIRTHEHDMECRFFRGVFNNCPFHLEEVGKYSTCFIWQLVFIRMNADSIDIMNWNYRVVYLMVYSKLMMCWDPYAAPPCFIEFPIQFGCYLQSQWYSATQLYDVFTPDMVWYWNQLERWLKEISPHEARSPEWTSWCHYGCCWMYQIQEQQNCQAKTPLA\n",
            "PDQMAPDEDAPPAEMGFQHYKVGIEFSMASSYDIKIFTYFPIELTSRWEQNLAPSMVIYIAQINDGTTTFPCTCDHHYTEDHPWCKTIMPPLRPISHEQQLSKDKKQFRYSPHLYCKPTICIFGYVNLGWLSNNVFY\n",
            "EPTLRDSDDSNVGSCRHEHQQVQATNNKFTEGYPDMNTVMSYEEYLHHMDTWFIPRMNATTYNHDDGVPCWNHHDLQDKAFYKCAQTCHMENNPGWKSKYGLNPHIYKDGMVKTFHACSQLNIDPAAMPFDDAALNVLRQ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### datasets"
      ],
      "metadata": {
        "id": "lAAHts2YyYxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "1xMZflIi0wBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = ProtT5EncodingModule()\n",
        "\n",
        "latent_list = []\n",
        "pseudotime_list = []\n",
        "sequence_list = []\n",
        "\n",
        "for cell_type, latents in scvi_latents.items():\n",
        "    pseudotimes = scvi_pseudotimes[cell_type]\n",
        "    for latent, pseudotime, sequence in zip(latents, pseudotimes, protein_sequences[cell_type]):\n",
        "        print(f\"Cell type: {cell_type}, Latent shape: {latents.shape}, Sequence length: {len(sequence)}, Pseudotime shape: {pseudotimes.shape}\")\n",
        "        latent_list.append(latents)\n",
        "        pseudotime_list.append(pseudotimes)\n",
        "        sequence_list.append(sequence)\n",
        "\n",
        "# seqs encoding protT5\n",
        "sequence_tensor_list = []\n",
        "for sequence in sequence_list:\n",
        "    sequence_tensor = encoder(sequence)\n",
        "    sequence_tensor_list.append(sequence_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc9X0I6-0YQq",
        "outputId": "296b6337-50fc-4263-fa9c-e2076cd940ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell type: macrophage, Latent shape: (50663, 50), Sequence length: 169, Pseudotime shape: (50663,)\n",
            "Cell type: macrophage, Latent shape: (50663, 50), Sequence length: 137, Pseudotime shape: (50663,)\n",
            "Cell type: macrophage, Latent shape: (50663, 50), Sequence length: 140, Pseudotime shape: (50663,)\n",
            "Cell type: monocyte, Latent shape: (27973, 50), Sequence length: 188, Pseudotime shape: (27973,)\n",
            "Cell type: monocyte, Latent shape: (27973, 50), Sequence length: 147, Pseudotime shape: (27973,)\n",
            "Cell type: monocyte, Latent shape: (27973, 50), Sequence length: 118, Pseudotime shape: (27973,)\n",
            "Cell type: endothelial cell of hepatic sinusoid, Latent shape: (15880, 50), Sequence length: 113, Pseudotime shape: (15880,)\n",
            "Cell type: endothelial cell of hepatic sinusoid, Latent shape: (15880, 50), Sequence length: 128, Pseudotime shape: (15880,)\n",
            "Cell type: endothelial cell of hepatic sinusoid, Latent shape: (15880, 50), Sequence length: 105, Pseudotime shape: (15880,)\n",
            "Cell type: liver dendritic cell, Latent shape: (15493, 50), Sequence length: 104, Pseudotime shape: (15493,)\n",
            "Cell type: liver dendritic cell, Latent shape: (15493, 50), Sequence length: 141, Pseudotime shape: (15493,)\n",
            "Cell type: liver dendritic cell, Latent shape: (15493, 50), Sequence length: 166, Pseudotime shape: (15493,)\n",
            "Cell type: nk cell, Latent shape: (24315, 50), Sequence length: 109, Pseudotime shape: (24315,)\n",
            "Cell type: nk cell, Latent shape: (24315, 50), Sequence length: 123, Pseudotime shape: (24315,)\n",
            "Cell type: nk cell, Latent shape: (24315, 50), Sequence length: 128, Pseudotime shape: (24315,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_tensor_list[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g6OvPn_1Dpx",
        "outputId": "e172ccef-5a1f-4790-dd8f-238fc1fb8a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 170, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# pad\n",
        "max_len = max(sequence.shape[1] for sequence in sequence_tensor_list)\n",
        "padded_sequence_tensor_list = [\n",
        "    torch.cat([sequence, torch.zeros(1, max_len - sequence.shape[1], sequence.shape[2], dtype=sequence.dtype)], dim=1)\n",
        "    if sequence.shape[1] < max_len else sequence for sequence in sequence_tensor_list\n",
        "]\n",
        "sequence_tensor = torch.cat(padded_sequence_tensor_list, dim=0)\n"
      ],
      "metadata": {
        "id": "Bqj90ko108cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vUDzJ6h3g0d",
        "outputId": "1f940b41-aec3-4002-a556-cd5098d72625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15, 189, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_list[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7_YDnHv6eYw",
        "outputId": "c578d2aa-64ba-4830-a913-785e9c3ec270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50663, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_tensor_list = [torch.tensor(latent, dtype=torch.float32) for latent in latent_list]\n",
        "\n",
        "max_len = max(latent.shape[0] for latent in latent_tensor_list)\n",
        "\n",
        "padded_latent_list = [\n",
        "    torch.cat([latent, torch.zeros((max_len - latent.shape[0], latent.shape[1]), dtype=latent.dtype)], dim=0)\n",
        "    if latent.shape[0] < max_len else latent for latent in latent_tensor_list\n",
        "]\n",
        "\n",
        "# instead of padding (low compute -> project sequence layer down to 1024 dim and train (current at 50K))\n",
        "class SequenceProjector(nn.Module):\n",
        "    def __init__(self, seq_len, d_output=1024, d_hidden=256):\n",
        "        super().__init__()\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(seq_len, d_hidden),\n",
        "            nn.LayerNorm(d_hidden),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(d_hidden, d_hidden),\n",
        "            nn.LayerNorm(d_hidden),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(d_hidden, d_output)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, emb_dim)\n",
        "        batch_size, seq_len, emb_dim = x.shape\n",
        "\n",
        "        # Transpose to (batch_size, emb_dim, seq_len)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # Project sequence length to d_output\n",
        "        x = self.projector(x)  # Shape: (batch_size, emb_dim, d_output)\n",
        "\n",
        "        # Transpose back to (batch_size, d_output, emb_dim)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Usage\n",
        "seq_len = padded_latent_list[0].shape[0]  # Original sequence length\n",
        "d_output = 1024  # Desired output sequence length\n",
        "emb_dim = padded_latent_list[0].shape[1]  # Original embedding dimension (50)\n",
        "\n",
        "sequence_projector = SequenceProjector(seq_len, d_output)\n",
        "\n",
        "projected_latents = []\n",
        "for latent in padded_latent_list:\n",
        "    projected = sequence_projector(latent.unsqueeze(0))  # Add batch dimension if needed\n",
        "    projected_latents.append(projected.squeeze(0))  # Remove batch dimension if added\n",
        "\n",
        "latent_tensor = torch.stack(projected_latents, dim=0)\n",
        "\n",
        "# latent_tensor = torch.stack(padded_latent_list, dim=0)\n"
      ],
      "metadata": {
        "id": "Funv8saA2pIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvUO7Z3K3xES",
        "outputId": "c4ad661a-49ad-40a9-f224-5bb94d24a967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15, 1024, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### pseudotime as a context:\n",
        "adata_list[0].obs['dpt_pseudotime'].shape\n",
        "unique_counts = adata_list[0].obs['cell_ontology_class'].value_counts()\n",
        "print(unique_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS86ezG-3Rns",
        "outputId": "b9c1f072-45f7-420e-f950-1f798fa3a44c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cell_ontology_class\n",
            "macrophage               35204\n",
            "mesenchymal stem cell    15459\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pseudotime_tensor_list = [torch.tensor(pseudotime, dtype=torch.float32) for pseudotime in pseudotime_list]\n",
        "\n",
        "# ensure pseudotime tensors have shape (seqlen, 1) before padding\n",
        "pseudotime_tensor_list = [\n",
        "    pseudotime if len(pseudotime.shape) == 2 else pseudotime.unsqueeze(1) for pseudotime in pseudotime_tensor_list\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvKatCSV70-m",
        "outputId": "bb9ba78e-4183-4c95-e742-08c058fc4596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-f91bad088d9a>:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  pseudotime_tensor_list = [torch.tensor(pseudotime, dtype=torch.float32) for pseudotime in pseudotime_list]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_pseudotime_len = max(pseudotime.shape[0] for pseudotime in pseudotime_tensor_list)\n",
        "\n",
        "padded_pseudotime_tensor_list = [\n",
        "    torch.cat([pseudotime, torch.zeros((max_pseudotime_len - pseudotime.shape[0], pseudotime.shape[1]), dtype=pseudotime.dtype)], dim=0)\n",
        "    if pseudotime.shape[0] < max_pseudotime_len else pseudotime for pseudotime in pseudotime_tensor_list\n",
        "]\n",
        "\n",
        "pseudotime_tensor = torch.stack(padded_pseudotime_tensor_list, dim=0)\n",
        "\n",
        "class SequenceProjector(nn.Module):\n",
        "    def __init__(self, seq_len, d_output=1024, d_hidden=256):\n",
        "        super().__init__()\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(seq_len, d_hidden),\n",
        "            nn.LayerNorm(d_hidden),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(d_hidden, d_hidden),\n",
        "            nn.LayerNorm(d_hidden),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(d_hidden, d_output)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, emb_dim)\n",
        "        batch_size, seq_len, emb_dim = x.shape\n",
        "\n",
        "        # Transpose to (batch_size, emb_dim, seq_len)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # Project sequence length to d_output\n",
        "        x = self.projector(x)  # Shape: (batch_size, emb_dim, d_output)\n",
        "\n",
        "        # Transpose back to (batch_size, d_output, emb_dim)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Usage\n",
        "seq_len = padded_pseudotime_tensor_list[0].shape[0]  # Original sequence length\n",
        "d_output = 1024  # Desired output sequence length\n",
        "emb_dim = padded_pseudotime_tensor_list[0].shape[1]  # Original embedding dimension (50)\n",
        "\n",
        "sequence_projector = SequenceProjector(seq_len, d_output)\n",
        "\n",
        "projected_latents = []\n",
        "for latent in padded_pseudotime_tensor_list:\n",
        "    projected = sequence_projector(latent.unsqueeze(0))  # Add batch dimension if needed\n",
        "    projected_latents.append(projected.squeeze(0))  # Remove batch dimension if added\n",
        "\n",
        "pseudotime_tensor = torch.stack(projected_latents, dim=0)"
      ],
      "metadata": {
        "id": "NHiB_2nk8Fig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pseudotime_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlYN6aGh84XI",
        "outputId": "31f4b293-daae-4d45-8842-73bd90637fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15, 1024, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seq dim = 1024, latent dim = 50, pseudotime dim = 1"
      ],
      "metadata": {
        "id": "vGgHwRuM-Rdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "full_dataset = TensorDataset(latent_tensor, pseudotime_tensor, sequence_tensor)\n",
        "\n",
        "# split size calculation\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(0.7 * total_size)\n",
        "val_size = total_size - train_size\n",
        "\n",
        "# dataset split\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# dataloader creation\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False)"
      ],
      "metadata": {
        "id": "kdqzZHCk_HWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEu3MrI3_SC3",
        "outputId": "ee4378fb-e739-4282-a24b-39467fa2045f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7e6ab870a530>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_dataloader:\n",
        "    scvi, dpt, seq = batch\n",
        "    print(scvi.shape)\n",
        "    print(seq.shape)\n",
        "    print(dpt.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2D220eTz-0L",
        "outputId": "a9d599b9-2296-436e-9e8d-0d8a5c97ebbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1024, 50])\n",
            "torch.Size([2, 189, 1024])\n",
            "torch.Size([2, 1024, 1])\n",
            "torch.Size([2, 1024, 50])\n",
            "torch.Size([2, 189, 1024])\n",
            "torch.Size([2, 1024, 1])\n",
            "torch.Size([2, 1024, 50])\n",
            "torch.Size([2, 189, 1024])\n",
            "torch.Size([2, 1024, 1])\n",
            "torch.Size([2, 1024, 50])\n",
            "torch.Size([2, 189, 1024])\n",
            "torch.Size([2, 1024, 1])\n",
            "torch.Size([2, 1024, 50])\n",
            "torch.Size([2, 189, 1024])\n",
            "torch.Size([2, 1024, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### models"
      ],
      "metadata": {
        "id": "RpCeq3tdyZv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProteinFlowMatching(nn.Module):\n",
        "    def __init__(self, flow_matching, decoder):\n",
        "        super().__init__()\n",
        "        self.flow_matching = flow_matching\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, x, target, context):\n",
        "        # During training, we compute the loss from flow matching\n",
        "        return self.flow_matching(x, target, context)\n",
        "\n",
        "    def generate(self, x, context, num_steps=200):\n",
        "        # Generate latent representation using flow matching\n",
        "        latent = self.flow_matching.euler_sample(context, x.shape, guidance_scale=3.0)[0]\n",
        "\n",
        "        # Decode the latent representation to protein sequence\n",
        "        protein_sequence = self.decoder(latent, max_length=num_steps)\n",
        "\n",
        "        return protein_sequence"
      ],
      "metadata": {
        "id": "VwYmnDMhLbdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "unet = CustomUNet1D(\n",
        "    in_channels=50,  # Dimension of scVI latents\n",
        "    out_channels=1024,  # Dimension of protein embeddings\n",
        "    model_channels=64,\n",
        "    num_res_blocks=2,\n",
        "    attention_resolutions=(1,),\n",
        "    dropout=0.1,\n",
        "    channel_mult=(1, 2, 4, 8),\n",
        "    use_spatial_transformer=True,\n",
        "    transformer_depth=1,\n",
        "    context_dim=1,  # Dimension of pseudotime\n",
        ")\n",
        "flow_matching = FlowMatchingTrainer(unet, sample_N=25)\n",
        "decoder = ProtT5DecodingModule()  # vocab_size is the number of amino acids + special tokens\n",
        "model = ProteinFlowMatching(flow_matching, decoder)\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "_oRb8_B9wLnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train"
      ],
      "metadata": {
        "id": "yVlSfboYplb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_epochs = 1\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for scvi_latent, pseudotime, protein_seq in train_dataloader:\n",
        "        scvi_latent = scvi_latent.to(device)\n",
        "        pseudotime = pseudotime.to(device)\n",
        "        protein_seq = protein_seq.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(scvi_latent, protein_seq, pseudotime)  # Forward pass through ProteinFlowMatching\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for scvi_latent, pseudotime, protein_seq in val_dataloader:\n",
        "            scvi_latent = scvi_latent.to(device)\n",
        "            pseudotime = pseudotime.to(device)\n",
        "            protein_seq = protein_seq.to(device)\n",
        "\n",
        "            loss = model(scvi_latent, protein_seq, pseudotime)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save the best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCS9pXSrzHrB",
        "outputId": "7c7b6edf-b6e8-4c58-d5b9-603fbfa2f826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-6a975ce1cc78>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscvi_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotein_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpseudotime\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forward pass through ProteinFlowMatching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate new protein sequences\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    scvi_latent = torch.randn(2000, 50).to(device)  # Random scVI latent\n",
        "    pseudotime = torch.rand(2000, 1).to(device)  # Random pseudotime\n",
        "    generated_sequence = model.generate(scvi_latent, pseudotime)\n",
        "    print(\"Generated sequence:\", generated_sequence)"
      ],
      "metadata": {
        "id": "wxPhbYMMbOPR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}