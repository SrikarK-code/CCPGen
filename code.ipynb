{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GxlZK1SHkgl9",
        "b-njBgLzltjL",
        "X-e-EUQyklC2",
        "ht7z5O8ilEnK",
        "zOjxMpSTljVi",
        "21dEMxSOw1ex",
        "JsBKL442wzXI",
        "lAAHts2YyYxk"
      ],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## workflow"
      ],
      "metadata": {
        "id": "GxlZK1SHkgl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Input: scVI embeddings of cell states\n",
        "b) Output: Generated protein sequences\n",
        "\n",
        "Process Flow:\n",
        "\n",
        "scVI embeddings → UNetModel → Flow Matching → Protein Decoder → Protein Sequence\n",
        "\n",
        "1. Input Preparation:\n",
        "   - Start with scVI (single-cell Variational Inference) embeddings of cell states.\n",
        "   - These embeddings are high-dimensional vectors (e.g., 128 dimensions) representing cellular gene expression profiles.\n",
        "   - The scVI embeddings are generated using a separate scVI model trained on single-cell RNA sequencing data.\n",
        "\n",
        "2. Model Architecture Overview:\n",
        "   The system consists of several key components:\n",
        "   a) UNetModel: The core generative model\n",
        "   b) FlowMatchingTrainer: Manages the flow matching process\n",
        "   c) ProtT5Encoder: Encodes protein sequences (for training)\n",
        "   d) ProtT5Decoder: Decodes latent representations to protein sequences\n",
        "\n",
        "3. UNetModel Detailed Architecture:\n",
        "   3.1. Initialization:\n",
        "   - The UNetModel is initialized with parameters like input/output channels, number of ResBlocks, attention resolutions, etc.\n",
        "   - Key components are created: time embedder, input blocks, middle block, output blocks.\n",
        "\n",
        "   3.2. Time Embedding:\n",
        "   - Function: timestep_embedding\n",
        "   - Converts a scalar timestep to a high-dimensional vector using sinusoidal functions.\n",
        "   - This embedding is further processed through a small MLP (self.time_embed).\n",
        "\n",
        "   3.3. Input Blocks:\n",
        "   - A series of TimestepEmbedSequential modules, each containing:\n",
        "     a) ResBlock: Combines feature maps with time embeddings\n",
        "     b) AttentionBlock or SpatialTransformer: For self-attention mechanisms\n",
        "     c) Downsample: Reduces spatial dimensions (if applicable)\n",
        "\n",
        "   3.4. Middle Block:\n",
        "   - Contains ResBlocks and Attention mechanisms for global reasoning.\n",
        "\n",
        "   3.5. Output Blocks:\n",
        "   - Mirror the input blocks, but with Upsample layers instead of Downsample.\n",
        "   - Use skip connections from input blocks.\n",
        "\n",
        "   3.6. Final Output Layer:\n",
        "   - Normalization followed by a convolution to produce the output channels.\n",
        "\n",
        "4. Detailed Component Breakdown:\n",
        "   4.1. ResBlock:\n",
        "   - Residual block that processes features and incorporates time embeddings.\n",
        "   - Contains normalization layers, convolutions, and optional up/downsampling.\n",
        "   - Uses checkpoint function for memory-efficient backpropagation.\n",
        "\n",
        "   4.2. AttentionBlock:\n",
        "   - Self-attention mechanism allowing interaction between different parts of the sequence.\n",
        "   - Uses QKVAttention for efficient attention computation.\n",
        "\n",
        "   4.3. SpatialTransformer:\n",
        "   - More sophisticated attention mechanism with multiple transformer layers.\n",
        "   - Each layer contains self-attention and feed-forward networks.\n",
        "\n",
        "   4.4. CrossAttention:\n",
        "   - Attention mechanism that can attend to a separate context (used in SpatialTransformer).\n",
        "   - Splits input into query, key, and value before computing attention.\n",
        "\n",
        "   4.5. FeedForward:\n",
        "   - Simple feedforward network used in transformer blocks.\n",
        "   - Contains two linear layers with GELU activation and dropout.\n",
        "\n",
        "   4.6. Upsample and Downsample:\n",
        "   - Handle changes in spatial dimensions of feature maps.\n",
        "   - Use either interpolation or transposed convolutions.\n",
        "\n",
        "   4.7. GroupNorm32:\n",
        "   - Custom group normalization for improved training stability.\n",
        "\n",
        "   4.8. TimestepEmbedSequential:\n",
        "   - Sequential module that handles passing of timestep embeddings to appropriate submodules.\n",
        "\n",
        "5. FlowMatchingTrainer:\n",
        "   - Manages the training process of the UNetModel.\n",
        "   - Implements the forward process (adding noise) and reverse process (denoising).\n",
        "   - Uses a noise schedule to control the amount of noise added at each timestep.\n",
        "   - Computes loss based on the model's ability to predict the noise added.\n",
        "\n",
        "6. ProtT5Encoder (for training):\n",
        "   - Utilizes a pre-trained ProtT5 model to encode protein sequences into a latent space.\n",
        "   - Processes amino acid sequences into a high-dimensional representation.\n",
        "\n",
        "7. ProtT5Decoder (for inference):\n",
        "   - Converts latent representations back into amino acid sequences.\n",
        "   - Uses beam search or other decoding strategies to generate the final protein sequence.\n",
        "\n",
        "8. Training Process:\n",
        "   8.1. Data Preparation:\n",
        "   - Batch of scVI embeddings and corresponding protein sequences are loaded.\n",
        "   - Protein sequences are encoded using ProtT5Encoder.\n",
        "\n",
        "   8.2. Forward Pass:\n",
        "   - Random timesteps are generated for each sample in the batch.\n",
        "   - Noise is added to the encoded protein sequences based on the timesteps.\n",
        "   - The UNetModel processes the noisy encodings, conditioned on scVI embeddings and timesteps.\n",
        "\n",
        "   8.3. Loss Computation:\n",
        "   - The model's output is compared to the true noise added.\n",
        "   - Loss is calculated (usually mean squared error).\n",
        "\n",
        "   8.4. Backpropagation:\n",
        "   - Gradients are computed and model parameters are updated.\n",
        "\n",
        "9. Inference Process:\n",
        "   9.1. Start with an scVI embedding of a cell state.\n",
        "   9.2. Generate random noise as the starting point.\n",
        "   9.3. Gradually denoise using the UNetModel:\n",
        "      - For each timestep (from most noisy to least):\n",
        "        - Pass the current noisy sample through the UNetModel.\n",
        "        - Use the model's prediction to update the sample.\n",
        "   9.4. The final denoised representation is passed through the ProtT5Decoder.\n",
        "   9.5. The decoder outputs the generated protein sequence.\n",
        "\n",
        "10. Utility Functions:\n",
        "    - conv_nd: Creates 1D convolutions for our sequence data.\n",
        "    - zero_module: Initializes a module's parameters to zero.\n",
        "    - normalization: Applies GroupNorm32 normalization.\n",
        "    - checkpoint: Implements gradient checkpointing for memory efficiency.\n",
        "    - exists and default: Helper functions for handling optional parameters.\n",
        "\n",
        "Cool adaptations\n",
        "    - Adaptation of 2D UNet architecture to 1D protein sequences.\n",
        "        - Unet in original model was used for the audio representations (spectograms)\n",
        "        - While we do use protein embeddings (like those from ProtT5), the UNet in our case still operates on a 1D sequence of these embeddings.\n",
        "        - Each position in this sequence corresponds to an amino acid, but is represented by a high-dimensional vector.\n",
        "        - The UNet processes this sequence of vectors, maintaining the 1D structure of the protein\n",
        "            - each element of the sequence is itself a rich high-dimensional representation (1280)\n",
        "    - Integration of flow matching with protein language models.\n",
        "    - Use of scVI embeddings as conditional input for targeted protein generation.\n",
        "\n",
        "\n",
        "Loss:\n",
        "θ^ = argmin_θ E_t,z_t ||u_θ(z_t, t, c) - v_t||^2\n",
        "Where:\n",
        "\n",
        "u_θ is your flow matching model (UNet)\n",
        "z_t is the scVI embedding at time t\n",
        "t is the timestep\n",
        "c is your context (which we'll discuss next)\n",
        "v_t is the target velocity (z_1 - (1-σ_min)z_0 in the optimal transport formulation)\n",
        "\n",
        "\n",
        "UNet\n",
        "This 1D UNet processes the scVI latent representations, which encode cellular states, through a series of downsampling and upsampling operations.\n",
        "The input blocks progressively reduce the spatial dimensions while increasing the channel depth, capturing hierarchical features.\n",
        "The middle block, with its deep channel representation, allows for global reasoning across the entire sequence.\n",
        "The output blocks then gradually upsample the representation back to the original dimensions, utilizing skip connections to preserve fine-grained information.\n",
        "Time embeddings are crucial, allowing the model to understand its position in the generation process.\n",
        "These embeddings are added to the input at each step, guiding the transformation from noise to protein sequence.\n",
        "Attention mechanisms, implemented either as AttentionBlocks or SpatialTransformers, enable the model to capture long-range dependencies critical for protein structure.\n",
        "The ResBlocks incorporate both the current state and the time embedding, allowing for time-dependent processing at each level.\n",
        "The context dimension, which could include additional information like pseudotime or motif data, is integrated through the SpatialTransformer blocks, providing extra conditioning for the generation process.\n",
        "The model's output represents the velocity field in the flow matching framework, predicting how the latent representation should change at each step to transform noise into a meaningful protein sequence representation.\n",
        "\n"
      ],
      "metadata": {
        "id": "vGFeB3nUkW7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## library installs"
      ],
      "metadata": {
        "id": "b-njBgLzltjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch anndata scvi-tools einops numpy scipy transformers scanpy"
      ],
      "metadata": {
        "id": "P6I50Tquluub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "828645c0-b579-4ff4-ed8e-f09449126616"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Collecting anndata\n",
            "  Downloading anndata-0.10.8-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting scvi-tools\n",
            "  Downloading scvi_tools-1.1.5-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Collecting scanpy\n",
            "  Downloading scanpy-1.10.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting array-api-compat!=1.5,>1.4 (from anndata)\n",
            "  Downloading array_api_compat-1.8-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata) (1.2.2)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from anndata) (3.11.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from anndata) (8.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from anndata) (24.1)\n",
            "Requirement already satisfied: pandas!=2.1.0rc0,!=2.1.2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from anndata) (2.1.4)\n",
            "Collecting docrep>=0.3.2 (from scvi-tools)\n",
            "  Downloading docrep-0.3.2.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.8.4)\n",
            "Requirement already satisfied: jax>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.4.26)\n",
            "Requirement already satisfied: jaxlib>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.4.26+cuda12.cudnn89)\n",
            "Collecting lightning<2.2,>=2.0 (from scvi-tools)\n",
            "  Downloading lightning-2.1.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-collections>=0.1.1 (from scvi-tools)\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mudata>=0.1.2 (from scvi-tools)\n",
            "  Downloading mudata-0.3.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting numpyro>=0.12.1 (from scvi-tools)\n",
            "  Downloading numpyro-0.15.2-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.2.2)\n",
            "Collecting pyro-ppl>=1.6.0 (from scvi-tools)\n",
            "  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (13.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (1.3.2)\n",
            "Collecting torchmetrics>=0.11.0 (from scvi-tools)\n",
            "  Downloading torchmetrics-1.4.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tqdm>=4.56.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (4.66.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.4.2)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.7.1)\n",
            "Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.5.6)\n",
            "Collecting pynndescent>=0.5 (from scanpy)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.13.1)\n",
            "Collecting session-info (from scanpy)\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.14.2)\n",
            "Collecting umap-learn!=0.5.0,>=0.5 (from scanpy)\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from docrep>=0.3.2->scvi-tools) (1.16.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.4->scvi-tools) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.4->scvi-tools) (3.3.0)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning<2.2,>=2.0->scvi-tools)\n",
            "  Downloading lightning_utilities-0.11.6-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting pytorch-lightning (from lightning<2.2,>=2.0->scvi-tools)\n",
            "  Downloading pytorch_lightning-2.3.3-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml-collections>=0.1.1->scvi-tools) (1.4.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml-collections>=0.1.1->scvi-tools) (21.6.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.56->scanpy) (0.43.0)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from numpyro>=0.12.1->scvi-tools) (1.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.1)\n",
            "Collecting pyro-api>=0.1.1 (from pyro-ppl>=1.6.0->scvi-tools)\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->scvi-tools) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->scvi-tools) (2.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.2->scvi-tools) (3.5.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax->scvi-tools) (1.0.8)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax->scvi-tools) (0.5.23)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax->scvi-tools) (0.1.63)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax->scvi-tools) (0.1.86)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Collecting stdlib_list (from session-info->scanpy)\n",
            "  Downloading stdlib_list-0.10.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax->scvi-tools) (0.12.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning<2.2,>=2.0->scvi-tools) (71.0.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->scvi-tools) (0.1.2)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->scvi-tools) (1.7.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->scvi-tools) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->scvi-tools) (3.20.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (4.0.3)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->scvi-tools) (6.4.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->scvi-tools) (3.19.2)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading anndata-0.10.8-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.4/124.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scvi_tools-1.1.5-py3-none-any.whl (387 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.7/387.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scanpy-1.10.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading array_api_compat-1.8-py3-none-any.whl (38 kB)\n",
            "Downloading legacy_api_wrap-1.4-py3-none-any.whl (15 kB)\n",
            "Downloading lightning-2.1.4-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mudata-0.3.0-py3-none-any.whl (39 kB)\n",
            "Downloading numpyro-0.15.2-py3-none-any.whl (348 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.1/348.1 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.4.1-py3-none-any.whl (866 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.2/866.2 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.6-py3-none-any.whl (26 kB)\n",
            "Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.3.3-py3-none-any.whl (812 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stdlib_list-0.10.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docrep, ml-collections, session-info\n",
            "  Building wheel for docrep (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docrep: filename=docrep-0.3.2-py3-none-any.whl size=19876 sha256=7405f74d94fe2463deb90808705c537160b966359c79d8a50557119ab6fbc49b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/64/48/03c38d8d906159eaa210b3c548fdb590eb3e2a4a5745ae2172\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94506 sha256=c53e93333567cd5fb800327b3a186d46893b3a59431c585c91f89ed169b0207f\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8023 sha256=3bb4cd1e815d1e7545bfdbc4bfd946188eb99ecd491d3a783c3f9a16d7e51bdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
            "Successfully built docrep ml-collections session-info\n",
            "Installing collected packages: pyro-api, array-api-compat, stdlib_list, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ml-collections, lightning-utilities, legacy-api-wrap, einops, docrep, session-info, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pynndescent, nvidia-cusolver-cu12, numpyro, anndata, umap-learn, mudata, torchmetrics, scanpy, pyro-ppl, pytorch-lightning, lightning, scvi-tools\n",
            "Successfully installed anndata-0.10.8 array-api-compat-1.8 docrep-0.3.2 einops-0.8.0 legacy-api-wrap-1.4 lightning-2.1.4 lightning-utilities-0.11.6 ml-collections-0.1.1 mudata-0.3.0 numpyro-0.15.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 pynndescent-0.5.13 pyro-api-0.1.2 pyro-ppl-1.9.1 pytorch-lightning-2.3.3 scanpy-1.10.2 scvi-tools-1.1.5 session-info-1.0.0 stdlib_list-0.10.0 torchmetrics-1.4.1 umap-learn-0.5.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils"
      ],
      "metadata": {
        "id": "X-e-EUQyklC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) 2023 Amphion.\n",
        "#\n",
        "# This source code is licensed under the MIT license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class AbstractDistribution:\n",
        "    def sample(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def mode(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "class DiracDistribution(AbstractDistribution):\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "\n",
        "    def sample(self):\n",
        "        return self.value\n",
        "\n",
        "    def mode(self):\n",
        "        return self.value\n",
        "\n",
        "\n",
        "class DiagonalGaussianDistribution(object):\n",
        "    def __init__(self, parameters, deterministic=False):\n",
        "        self.parameters = parameters\n",
        "        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n",
        "        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n",
        "        self.deterministic = deterministic\n",
        "        self.std = torch.exp(0.5 * self.logvar)\n",
        "        self.var = torch.exp(self.logvar)\n",
        "        if self.deterministic:\n",
        "            self.var = self.std = torch.zeros_like(self.mean).to(\n",
        "                device=self.parameters.device\n",
        "            )\n",
        "\n",
        "    def sample(self):\n",
        "        x = self.mean + self.std * torch.randn(self.mean.shape).to(\n",
        "            device=self.parameters.device\n",
        "        )\n",
        "        return x\n",
        "\n",
        "    def kl(self, other=None):\n",
        "        if self.deterministic:\n",
        "            return torch.Tensor([0.0])\n",
        "        else:\n",
        "            if other is None:\n",
        "                return 0.5 * torch.sum(\n",
        "                    torch.pow(self.mean, 2) + self.var - 1.0 - self.logvar,\n",
        "                    dim=[1, 2, 3],\n",
        "                )\n",
        "            else:\n",
        "                return 0.5 * torch.sum(\n",
        "                    torch.pow(self.mean - other.mean, 2) / other.var\n",
        "                    + self.var / other.var\n",
        "                    - 1.0\n",
        "                    - self.logvar\n",
        "                    + other.logvar,\n",
        "                    dim=[1, 2, 3],\n",
        "                )\n",
        "\n",
        "    def nll(self, sample, dims=[1, 2, 3]):\n",
        "        if self.deterministic:\n",
        "            return torch.Tensor([0.0])\n",
        "        logtwopi = np.log(2.0 * np.pi)\n",
        "        return 0.5 * torch.sum(\n",
        "            logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var,\n",
        "            dim=dims,\n",
        "        )\n",
        "\n",
        "    def mode(self):\n",
        "        return self.mean\n",
        "\n",
        "\n",
        "def normal_kl(mean1, logvar1, mean2, logvar2):\n",
        "    \"\"\"\n",
        "    source: https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/losses.py#L12\n",
        "    Compute the KL divergence between two gaussians.\n",
        "    Shapes are automatically broadcasted, so batches can be compared to\n",
        "    scalars, among other use cases.\n",
        "    \"\"\"\n",
        "    tensor = None\n",
        "    for obj in (mean1, logvar1, mean2, logvar2):\n",
        "        if isinstance(obj, torch.Tensor):\n",
        "            tensor = obj\n",
        "            break\n",
        "    assert tensor is not None, \"at least one argument must be a Tensor\"\n",
        "\n",
        "    # Force variances to be Tensors. Broadcasting helps convert scalars to\n",
        "    # Tensors, but it does not work for torch.exp().\n",
        "    logvar1, logvar2 = [\n",
        "        x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor)\n",
        "        for x in (logvar1, logvar2)\n",
        "    ]\n",
        "\n",
        "    return 0.5 * (\n",
        "        -1.0\n",
        "        + logvar2\n",
        "        - logvar1\n",
        "        + torch.exp(logvar1 - logvar2)\n",
        "        + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "g37xH4c6kO8O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractmethod\n",
        "from functools import partial\n",
        "import math\n",
        "from typing import Iterable\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from einops import repeat\n",
        "def Normalize(in_channels):\n",
        "    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "def count_flops_attn(model, _x, y):\n",
        "    b, c, *spatial = y[0].shape\n",
        "    num_spatial = int(np.prod(spatial))\n",
        "    matmul_ops = 2 * b * (num_spatial**2) * c\n",
        "    model.total_ops += torch.DoubleTensor([matmul_ops])\n",
        "\n",
        "def timestep_embedding(timesteps, dim, max_period=10000):\n",
        "    half = dim // 2\n",
        "    freqs = torch.exp(\n",
        "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
        "    ).to(device=timesteps.device)\n",
        "    args = timesteps[:, None].float() * freqs[None]\n",
        "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "    if dim % 2:\n",
        "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "    return embedding\n",
        "\n",
        "def conv_nd(dims, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Create a 1D, 2D, or 3D convolution module.\n",
        "    \"\"\"\n",
        "    if dims == 1:\n",
        "        return nn.Conv1d(*args, **kwargs)\n",
        "    elif dims == 2:\n",
        "        return nn.Conv2d(*args, **kwargs)\n",
        "    elif dims == 3:\n",
        "        return nn.Conv3d(*args, **kwargs)\n",
        "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
        "\n",
        "def linear(*args, **kwargs):\n",
        "    \"\"\"\n",
        "    Create a linear module.\n",
        "    \"\"\"\n",
        "    return nn.Linear(*args, **kwargs)\n",
        "\n",
        "def avg_pool_nd(dims, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Create a 1D, 2D, or 3D average pooling module.\n",
        "    \"\"\"\n",
        "    if dims == 1:\n",
        "        return nn.AvgPool1d(*args, **kwargs)\n",
        "    elif dims == 2:\n",
        "        return nn.AvgPool2d(*args, **kwargs)\n",
        "    elif dims == 3:\n",
        "        return nn.AvgPool3d(*args, **kwargs)\n",
        "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"\n",
        "    Zero out the parameters of a module and return it.\n",
        "    \"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "def normalization(channels):\n",
        "    \"\"\"\n",
        "    Make a standard normalization layer.\n",
        "    :param channels: number of input channels.\n",
        "    :return: an nn.Module for normalization.\n",
        "    \"\"\"\n",
        "    return GroupNorm32(32, channels)\n",
        "\n",
        "class GroupNorm32(nn.GroupNorm):\n",
        "    def forward(self, x):\n",
        "        return super().forward(x.float()).type(x.dtype)\n",
        "\n",
        "def checkpoint(func, inputs, params, flag):\n",
        "    \"\"\"\n",
        "    Evaluate a function without caching intermediate activations, allowing for\n",
        "    reduced memory at the expense of extra compute in the backward pass.\n",
        "    \"\"\"\n",
        "    if flag:\n",
        "        args = tuple(inputs) + tuple(params)\n",
        "        return CheckpointFunction.apply(func, len(inputs), *args)\n",
        "    else:\n",
        "        return func(*inputs)\n",
        "\n",
        "class TimestepBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Any module where forward() takes timestep embeddings as a second argument.\n",
        "    \"\"\"\n",
        "    @abstractmethod\n",
        "    def forward(self, x, emb):\n",
        "        \"\"\"\n",
        "        Apply the module to `x` given `emb` timestep embeddings.\n",
        "        \"\"\"\n",
        "\n",
        "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
        "    \"\"\"\n",
        "    A sequential module that passes timestep embeddings to the children that\n",
        "    support it as an extra input.\n",
        "    \"\"\"\n",
        "    def forward(self, x, emb, context=None):\n",
        "        for layer in self:\n",
        "            if isinstance(layer, TimestepBlock):\n",
        "                x = layer(x, emb)\n",
        "            elif isinstance(layer, SpatialTransformer):\n",
        "                x = layer(x, context)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "nAl6JpQjkO-k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = default(context_dim, query_dim)\n",
        "\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
        "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, query_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, context=None, mask=None):\n",
        "        h = self.heads\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        context = default(context, x)\n",
        "        k = self.to_k(context)\n",
        "        v = self.to_v(context)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
        "\n",
        "        sim = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = rearrange(mask, 'b ... -> b (...)')\n",
        "            max_neg_value = -torch.finfo(sim.dtype).max\n",
        "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
        "            sim.masked_fill_(~mask, max_neg_value)\n",
        "\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum('b i j, b j d -> b i d', attn, v)\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.0):\n",
        "        super().__init__()\n",
        "        inner_dim = int(dim * mult)\n",
        "        dim_out = default(dim_out, dim)\n",
        "        project_in = nn.Sequential(\n",
        "            nn.Linear(dim, inner_dim),\n",
        "            nn.GELU()\n",
        "        ) if not glu else GEGLU(dim, inner_dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            project_in,\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(inner_dim, dim_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if callable(d) else d\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
        "        return x * F.gelu(gate)\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, channels, use_conv, dims=1, out_channels=None, padding=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.dims = dims\n",
        "        if use_conv:\n",
        "            self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        if self.dims == 3:\n",
        "            x = F.interpolate(x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\")\n",
        "        else:\n",
        "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if self.use_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, channels, use_conv, dims=1, out_channels=None, padding=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.dims = dims\n",
        "        stride = 2 if dims != 3 else (1, 2, 2)\n",
        "        if use_conv:\n",
        "            self.op = conv_nd(dims, self.channels, self.out_channels, 3, stride=stride, padding=padding)\n",
        "        else:\n",
        "            assert self.channels == self.out_channels\n",
        "            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        return self.op(x)\n",
        "\n",
        "class ResBlock(TimestepBlock):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        emb_channels,\n",
        "        dropout,\n",
        "        out_channels=None,\n",
        "        use_conv=False,\n",
        "        use_scale_shift_norm=False,\n",
        "        dims=1,\n",
        "        use_checkpoint=False,\n",
        "        up=False,\n",
        "        down=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.emb_channels = emb_channels\n",
        "        self.dropout = dropout\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.use_scale_shift_norm = use_scale_shift_norm\n",
        "\n",
        "        self.in_layers = nn.Sequential(\n",
        "            normalization(channels),\n",
        "            nn.SiLU(),\n",
        "            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.updown = up or down\n",
        "\n",
        "        if up:\n",
        "            self.h_upd = Upsample(channels, False, dims)\n",
        "            self.x_upd = Upsample(channels, False, dims)\n",
        "        elif down:\n",
        "            self.h_upd = Downsample(channels, False, dims)\n",
        "            self.x_upd = Downsample(channels, False, dims)\n",
        "        else:\n",
        "            self.h_upd = self.x_upd = nn.Identity()\n",
        "\n",
        "        self.emb_layers = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            linear(\n",
        "                emb_channels,\n",
        "                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n",
        "            ),\n",
        "        )\n",
        "        self.out_layers = nn.Sequential(\n",
        "            normalization(self.out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            zero_module(conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)),\n",
        "        )\n",
        "\n",
        "        if self.out_channels == channels:\n",
        "            self.skip_connection = nn.Identity()\n",
        "        elif use_conv:\n",
        "            self.skip_connection = conv_nd(dims, channels, self.out_channels, 3, padding=1)\n",
        "        else:\n",
        "            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n",
        "\n",
        "    def forward(self, x, emb):\n",
        "        return checkpoint(self._forward, (x, emb), self.parameters(), self.use_checkpoint)\n",
        "\n",
        "    def _forward(self, x, emb):\n",
        "        if self.updown:\n",
        "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
        "            h = in_rest(x)\n",
        "            h = self.h_upd(h)\n",
        "            x = self.x_upd(x)\n",
        "            h = in_conv(h)\n",
        "        else:\n",
        "            h = self.in_layers(x)\n",
        "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
        "        while len(emb_out.shape) < len(h.shape):\n",
        "            emb_out = emb_out[..., None]\n",
        "        if self.use_scale_shift_norm:\n",
        "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
        "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
        "            h = out_norm(h) * (1 + scale) + shift\n",
        "            h = out_rest(h)\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "        return self.skip_connection(x) + h\n",
        "\n",
        "class QKVAttention(nn.Module):\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\n",
        "            \"bct,bcs->bts\",\n",
        "            (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),\n",
        "        )\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        num_heads=1,\n",
        "        num_head_channels=-1,\n",
        "        use_checkpoint=False,\n",
        "        use_new_attention_order=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        if num_head_channels == -1:\n",
        "            self.num_heads = num_heads\n",
        "        else:\n",
        "            assert channels % num_head_channels == 0\n",
        "            self.num_heads = channels // num_head_channels\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.norm = normalization(channels)\n",
        "        self.qkv = conv_nd(1, channels, channels * 3, 1)\n",
        "        self.attention = QKVAttention(self.num_heads)\n",
        "        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return checkpoint(self._forward, (x,), self.parameters(), self.use_checkpoint)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        b, c, *spatial = x.shape\n",
        "        x = x.reshape(b, c, -1)\n",
        "        qkv = self.qkv(self.norm(x))\n",
        "        h = self.attention(qkv)\n",
        "        h = self.proj_out(h)\n",
        "        return (x + h).reshape(b, c, *spatial)\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self, in_channels, n_heads, d_head, depth=1, dropout=0., context_dim=None):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        inner_dim = n_heads * d_head\n",
        "        self.norm = Normalize(in_channels)\n",
        "\n",
        "        self.proj_in = nn.Conv1d(in_channels, inner_dim, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [BasicTransformerBlock(inner_dim, n_heads, d_head, dropout=dropout, context_dim=context_dim)\n",
        "             for d in range(depth)]\n",
        "        )\n",
        "\n",
        "        self.proj_out = zero_module(nn.Conv1d(inner_dim, in_channels, kernel_size=1, stride=1, padding=0))\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        # note: if no context is given, cross-attention defaults to self-attention\n",
        "        b, c, s = x.shape\n",
        "        x_in = x\n",
        "        x = self.norm(x)\n",
        "        x = self.proj_in(x)\n",
        "        x = rearrange(x, 'b c s -> b s c')\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, context=context)\n",
        "        x = rearrange(x, 'b s c -> b c s')\n",
        "        x = self.proj_out(x)\n",
        "        return x + x_in\n",
        "\n",
        "class BasicTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, n_heads, d_head, dropout=0., context_dim=None, gated_ff=True, checkpoint=True):\n",
        "        super().__init__()\n",
        "        self.attn1 = CrossAttention(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout)\n",
        "        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)\n",
        "        self.attn2 = CrossAttention(query_dim=dim, context_dim=context_dim,\n",
        "                                    heads=n_heads, dim_head=d_head, dropout=dropout)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "        self.checkpoint = checkpoint\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        return checkpoint(self._forward, (x, context), self.parameters(), self.checkpoint)\n",
        "\n",
        "    def _forward(self, x, context=None):\n",
        "        x = self.attn1(self.norm1(x)) + x\n",
        "        x = self.attn2(self.norm2(x), context=context) + x\n",
        "        x = self.ff(self.norm3(x)) + x\n",
        "        return x"
      ],
      "metadata": {
        "id": "w0P4txPYkPAp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pretrained embedding modules (prott5, scvi)"
      ],
      "metadata": {
        "id": "ht7z5O8ilEnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5EncoderModel, T5Tokenizer\n",
        "import torch.nn as nn\n",
        "import re\n",
        "\n",
        "class ProtT5EncodingModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.protT5_model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
        "        self.protT5_tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        processed_seq = \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))\n",
        "        ids = self.protT5_tokenizer(processed_seq, add_special_tokens=True, return_tensors=\"pt\", padding='longest')\n",
        "        input_ids = ids['input_ids'].to(self.protT5_model.device)\n",
        "        attention_mask = ids['attention_mask'].to(self.protT5_model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding_repr = self.protT5_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        seq_emb = embedding_repr.last_hidden_state\n",
        "        return seq_emb"
      ],
      "metadata": {
        "id": "AQRVEnwZlULP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "class ProtT5DecodingModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.protT5_model = T5ForConditionalGeneration.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "        self.protT5_tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
        "\n",
        "    def forward(self, latent_repr, max_length=200):\n",
        "        outputs = self.protT5_model.generate(\n",
        "            inputs_embeds=latent_repr,\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        decoded_sequences = self.protT5_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        return decoded_sequences"
      ],
      "metadata": {
        "id": "72smUTRJkPCv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scvi\n",
        "import scanpy as sc\n",
        "import numpy as np\n",
        "\n",
        "# encode pseudotime and latent representations\n",
        "\n",
        "class SCVIEncodingModule:\n",
        "    def __init__(self):\n",
        "        self.latent_representations = {}\n",
        "        self.pseudotime_representations = {}\n",
        "\n",
        "    def encode(self, adata_dict):\n",
        "        for cell_type, adata in adata_dict.items():\n",
        "            print(f\"Training and embedding for cell type: {cell_type}...\")\n",
        "\n",
        "            adata_copy = adata.copy()\n",
        "\n",
        "            nan_count = np.isnan(adata_copy.X).sum()\n",
        "            if nan_count > 0:\n",
        "                print(f\"There are {nan_count} NaN values in adata_copy.X for cell type: {cell_type}\")\n",
        "            else:\n",
        "                print(f\"No NaN values found in adata_copy.X for cell type: {cell_type}\")\n",
        "\n",
        "            latent = adata_copy.obsm['X_scvi']\n",
        "            pseudotime = adata_copy.obs['dpt_pseudotime']\n",
        "\n",
        "            # Store latent representation in the dictionary\n",
        "            self.latent_representations[cell_type] = latent\n",
        "            self.pseudotime_representations[cell_type] = pseudotime\n",
        "\n",
        "        print(\"Encoding completed.\")\n",
        "\n",
        "        return self.latent_representations, self.pseudotime_representations"
      ],
      "metadata": {
        "id": "OeMMOtkGuWQP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## unet model (flow matching)"
      ],
      "metadata": {
        "id": "PBEPeCXzk3zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomUNet1D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        model_channels,\n",
        "        num_res_blocks,\n",
        "        attention_resolutions,\n",
        "        dropout=0,\n",
        "        channel_mult=(1, 2, 4, 8),\n",
        "        conv_resample=True,\n",
        "        use_checkpoint=False,\n",
        "        num_heads=8,\n",
        "        use_scale_shift_norm=False,\n",
        "        use_spatial_transformer=False,\n",
        "        transformer_depth=1,\n",
        "        context_dim=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attention_resolutions = attention_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.channel_mult = channel_mult\n",
        "        self.conv_resample = conv_resample\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.num_heads = num_heads\n",
        "        self.use_spatial_transformer = use_spatial_transformer\n",
        "        self.context_dim = context_dim\n",
        "\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(model_channels, time_embed_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim),\n",
        "        )\n",
        "\n",
        "        self.input_blocks = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels, model_channels, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        ch = model_channels\n",
        "        input_block_chans = [model_channels]\n",
        "        ds = 1\n",
        "        for level, mult in enumerate(channel_mult):\n",
        "            for _ in range(num_res_blocks):\n",
        "                layers = [\n",
        "                    ResBlock1D(\n",
        "                        ch,\n",
        "                        time_embed_dim,\n",
        "                        dropout,\n",
        "                        out_channels=mult * model_channels,\n",
        "                        use_scale_shift_norm=use_scale_shift_norm,\n",
        "                    )\n",
        "                ]\n",
        "                ch = mult * model_channels\n",
        "                if ds in attention_resolutions:\n",
        "                    if use_spatial_transformer:\n",
        "                        layers.append(\n",
        "                            SpatialTransformer1D(\n",
        "                                ch, num_heads, context_dim, depth=transformer_depth\n",
        "                            )\n",
        "                        )\n",
        "                    else:\n",
        "                        layers.append(AttentionBlock1D(ch, num_heads=num_heads))\n",
        "                self.input_blocks.append(nn.Sequential(*layers))\n",
        "                input_block_chans.append(ch)\n",
        "            if level != len(channel_mult) - 1:\n",
        "                self.input_blocks.append(\n",
        "                    nn.Conv1d(ch, ch, 3, stride=2, padding=1)\n",
        "                )\n",
        "                input_block_chans.append(ch)\n",
        "                ds *= 2\n",
        "\n",
        "        self.middle_block = nn.Sequential(\n",
        "            ResBlock1D(\n",
        "                ch,\n",
        "                time_embed_dim,\n",
        "                dropout,\n",
        "                use_scale_shift_norm=use_scale_shift_norm,\n",
        "            ),\n",
        "            AttentionBlock1D(ch, num_heads=num_heads) if not use_spatial_transformer else\n",
        "            SpatialTransformer1D(ch, num_heads, context_dim, depth=transformer_depth),\n",
        "            ResBlock1D(\n",
        "                ch,\n",
        "                time_embed_dim,\n",
        "                dropout,\n",
        "                use_scale_shift_norm=use_scale_shift_norm,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.output_blocks = nn.ModuleList([])\n",
        "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
        "            for i in range(num_res_blocks + 1):\n",
        "                layers = [\n",
        "                    ResBlock1D(\n",
        "                        ch + input_block_chans.pop(),\n",
        "                        time_embed_dim,\n",
        "                        dropout,\n",
        "                        out_channels=model_channels * mult,\n",
        "                        use_scale_shift_norm=use_scale_shift_norm,\n",
        "                    )\n",
        "                ]\n",
        "                ch = model_channels * mult\n",
        "                if ds in attention_resolutions:\n",
        "                    if use_spatial_transformer:\n",
        "                        layers.append(\n",
        "                            SpatialTransformer1D(\n",
        "                                ch, num_heads, context_dim, depth=transformer_depth\n",
        "                            )\n",
        "                        )\n",
        "                    else:\n",
        "                        layers.append(AttentionBlock1D(ch, num_heads=num_heads))\n",
        "                if level and i == num_res_blocks:\n",
        "                    layers.append(nn.ConvTranspose1d(ch, ch, 4, stride=2, padding=1))\n",
        "                    ds //= 2\n",
        "                self.output_blocks.append(nn.Sequential(*layers))\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(32, ch),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv1d(ch, out_channels, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, timesteps, context=None):\n",
        "      x = x.transpose(1, 2)  # transpose shape: [2, 50, 50663]\n",
        "      t_emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
        "\n",
        "      h = x\n",
        "      hs = []\n",
        "      for module in self.input_blocks:\n",
        "          print(\"Module type input blocks:\", type(module))\n",
        "          if isinstance(module, nn.Conv1d):\n",
        "              h = module(h)\n",
        "          elif isinstance(module, SpatialTransformer1D):\n",
        "            h = module(h, context)\n",
        "          elif isinstance(module, ResBlock1D):\n",
        "            h = module(h, t_emb)\n",
        "          elif isinstance(module, nn.Sequential):\n",
        "              for submodule in module:\n",
        "                  print(\"submodule type input blocks:\", type(submodule))\n",
        "                  if isinstance(submodule, ResBlock1D):\n",
        "                      h = submodule(h, t_emb)\n",
        "                  elif isinstance(submodule, SpatialTransformer1D):\n",
        "                      h = submodule(h, context)\n",
        "                  else:\n",
        "                      h = submodule(h)\n",
        "          else:\n",
        "              h = module(h)\n",
        "          hs.append(h)\n",
        "\n",
        "      if isinstance(self.middle_block, nn.Sequential):\n",
        "          for submodule in self.middle_block:\n",
        "              if isinstance(submodule, ResBlock1D):\n",
        "                  h = submodule(h, t_emb)\n",
        "              elif isinstance(submodule, SpatialTransformer1D):\n",
        "                  h = submodule(h, context)\n",
        "              else:\n",
        "                  h = submodule(h)\n",
        "      else:\n",
        "          h = self.middle_block(h)\n",
        "\n",
        "      for module in self.output_blocks:\n",
        "          h = torch.cat([h, hs.pop()], dim=1)\n",
        "          if isinstance(module, nn.Sequential):\n",
        "              for submodule in module:\n",
        "                  if isinstance(submodule, ResBlock1D):\n",
        "                      h = submodule(h, t_emb)\n",
        "                  elif isinstance(submodule, SpatialTransformer1D):\n",
        "                      h = submodule(h, context)\n",
        "                  else:\n",
        "                      h = submodule(h)\n",
        "          elif isinstance(module, SpatialTransformer1D):\n",
        "            h = module(h, context)\n",
        "          elif isinstance(module, ResBlock1D):\n",
        "            h = module(h, t_emb)\n",
        "          else:\n",
        "              h = module(h)\n",
        "\n",
        "      output = self.out(h)\n",
        "      output = output.transpose(1, 2)  # shape: [2, 50663, 1024]\n",
        "\n",
        "      return output\n",
        "\n",
        "class ResBlock1D(nn.Module):\n",
        "    def __init__(self, channels, time_embed_dim, dropout, out_channels=None, use_scale_shift_norm=False):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.time_embed_dim = time_embed_dim\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_scale_shift_norm = use_scale_shift_norm\n",
        "\n",
        "        self.in_layers = nn.Sequential(\n",
        "            nn.GroupNorm(32, channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv1d(channels, self.out_channels, 3, padding=1),\n",
        "        )\n",
        "        self.emb_layers = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, 2 * self.out_channels if use_scale_shift_norm else self.out_channels),\n",
        "        )\n",
        "        self.out_layers = nn.Sequential(\n",
        "            nn.GroupNorm(32, self.out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv1d(self.out_channels, self.out_channels, 3, padding=1),\n",
        "        )\n",
        "        if channels != self.out_channels:\n",
        "            self.skip_connection = nn.Conv1d(channels, self.out_channels, 1)\n",
        "        else:\n",
        "            self.skip_connection = nn.Identity()\n",
        "\n",
        "    def forward(self, x, emb):\n",
        "        h = self.in_layers(x)\n",
        "        emb_out = self.emb_layers(emb).unsqueeze(2)\n",
        "        if self.use_scale_shift_norm:\n",
        "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
        "            h = self.out_layers[0](h) * (1 + scale) + shift\n",
        "            h = self.out_layers[1:](h)\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "        return self.skip_connection(x) + h\n",
        "\n",
        "class AttentionBlock1D(nn.Module):\n",
        "    def __init__(self, channels, num_heads=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.norm = nn.GroupNorm(32, channels)\n",
        "        self.qkv = nn.Conv1d(channels, channels * 3, 1)\n",
        "        self.attention = QKVAttention(num_heads)\n",
        "        self.proj_out = nn.Conv1d(channels, channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, s = x.shape\n",
        "        qkv = self.qkv(self.norm(x))\n",
        "        qkv = qkv.reshape(b * self.num_heads, -1, s)\n",
        "        h = self.attention(qkv)\n",
        "        h = h.reshape(b, -1, s)\n",
        "        return x + self.proj_out(h)\n",
        "\n",
        "class QKVAttention(nn.Module):\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        bs, width, length = qkv.shape\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\n",
        "            \"bct,bcs->bts\",\n",
        "            (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),\n",
        "        )\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "class SpatialTransformer1D(nn.Module):\n",
        "    def __init__(self, channels, num_heads, context_dim, depth=1):\n",
        "        super().__init__()\n",
        "        self.norm = nn.GroupNorm(32, channels)\n",
        "        inner_dim = channels\n",
        "        self.proj_in = nn.Conv1d(channels, inner_dim, 1)\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [BasicTransformerBlock(inner_dim, num_heads, context_dim) for _ in range(depth)]\n",
        "        )\n",
        "        self.proj_out = nn.Conv1d(inner_dim, channels, 1)\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        b, c, s = x.shape\n",
        "        x_in = x\n",
        "        x = self.norm(x)\n",
        "        x = self.proj_in(x)\n",
        "        x = x.permute(0, 2, 1).contiguous()\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, context)\n",
        "        x = x.permute(0, 2, 1).contiguous()\n",
        "        x = self.proj_out(x)\n",
        "        return x + x_in\n",
        "\n",
        "class BasicTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, context_dim):\n",
        "        super().__init__()\n",
        "        self.attn1 = CrossAttention(dim, dim, num_heads)\n",
        "        self.ff = FeedForward(dim)\n",
        "        self.attn2 = CrossAttention(dim, context_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        x = self.attn1(self.norm1(x)) + x\n",
        "        x = self.attn2(self.norm2(x), context=context) + x\n",
        "        x = self.ff(self.norm3(x)) + x\n",
        "        return x\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim, num_heads, dim_head=64):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * num_heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
        "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        h = self.num_heads\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        context = x if context is None else context\n",
        "        k = self.to_k(context)\n",
        "        v = self.to_v(context)\n",
        "\n",
        "        q, k, v = map(lambda t: t.reshape(t.shape[0], -1, h, t.shape[-1] // h).permute(0, 2, 1, 3), (q, k, v))\n",
        "        sim = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
        "        out = out.permute(0, 2, 1, 3).reshape(out.shape[0], -1, out.shape[-1] * h)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim * mult, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def timestep_embedding(timesteps, dim, max_period=10000):\n",
        "    half = dim // 2\n",
        "    freqs = torch.exp(\n",
        "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
        "    ).to(device=timesteps.device)\n",
        "    args = timesteps[:, None].float() * freqs[None]\n",
        "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "    if dim % 2:\n",
        "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "    return embedding\n",
        "\n",
        "class TimestepEmbedSequential(nn.Sequential):\n",
        "    def forward(self, x, emb, context=None):\n",
        "        for layer in self:\n",
        "            if isinstance(layer, ResBlock1D):\n",
        "                x = layer(x, emb)\n",
        "            elif isinstance(layer, SpatialTransformer1D):\n",
        "                x = layer(x, context)\n",
        "            elif isinstance(layer, nn.Conv1d) or isinstance(layer, nn.GroupNorm) or isinstance(layer, nn.ReLU):\n",
        "                x = layer(x)\n",
        "            else:\n",
        "                x = layer(x, emb, context)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vtTtnoVYk9B-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ode solver"
      ],
      "metadata": {
        "id": "zOjxMpSTljVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.integrate import solve_ivp\n",
        "import numpy as np\n",
        "\n",
        "class ODESolverModule:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def ode_func(self, t, y, *args):\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(0)\n",
        "        t_tensor = torch.tensor([t], dtype=torch.float32)\n",
        "        with torch.no_grad():\n",
        "            dy_dt = self.model(y_tensor, t_tensor, *args).squeeze().numpy()\n",
        "        return dy_dt\n",
        "\n",
        "    def solve(self, y0, t_span, *args, method='RK45', **kwargs):\n",
        "        solution = solve_ivp(\n",
        "            fun=lambda t, y: self.ode_func(t, y, *args),\n",
        "            t_span=t_span,\n",
        "            y0=y0,\n",
        "            method=method,\n",
        "            **kwargs\n",
        "        )\n",
        "        return solution"
      ],
      "metadata": {
        "id": "HN_dc7YsllVr"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## flow trainer"
      ],
      "metadata": {
        "id": "yQZhv_Qwk3Rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FlowMatchingTrainer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        init_type=\"gaussian\",\n",
        "        noise_scale=1.0,\n",
        "        reflow_t_schedule=\"uniform\",\n",
        "        use_ode_sampler=\"euler\",\n",
        "        sigma_var=0.0,\n",
        "        ode_tol=1e-5,\n",
        "        sample_N=25,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.init_type = init_type\n",
        "        self.noise_scale = noise_scale\n",
        "        self.reflow_t_schedule = reflow_t_schedule\n",
        "        self.use_ode_sampler = use_ode_sampler\n",
        "        self.sigma_var = sigma_var\n",
        "        self.ode_tol = ode_tol\n",
        "        self.sample_N = sample_N\n",
        "        self.T = 1\n",
        "        self.eps = 1e-3\n",
        "        self.sigma_t = lambda t: (1.0 - t) * sigma_var\n",
        "\n",
        "    def forward(self, x_0, c):\n",
        "        t = torch.rand(x_0.shape[0], device=x_0.device) * (self.T - self.eps) + self.eps\n",
        "        t_expand = t.view(-1, 1, 1).repeat(1, x_0.shape[1], x_0.shape[2])\n",
        "        c = c.to(x_0.device)\n",
        "\n",
        "        noise = torch.randn_like(x_0)\n",
        "        target = x_0 - noise\n",
        "        perturbed_data = t_expand * x_0 + (1 - t_expand) * noise\n",
        "\n",
        "        model_out = self.model(perturbed_data, t * 999, c)\n",
        "\n",
        "        loss = F.mse_loss(model_out, target, reduction=\"none\").mean([1, 2]).mean()\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def euler_sample(self, cond, shape, guidance_scale):\n",
        "        device = self.model.device\n",
        "        batch = torch.randn(shape, device=device)\n",
        "        x = torch.randn_like(batch)\n",
        "        dt = 1.0 / self.sample_N\n",
        "        eps = 1e-3\n",
        "        for i in range(self.sample_N):\n",
        "            num_t = i / self.sample_N * (self.T - eps) + eps\n",
        "            t = torch.ones(batch.shape[0], device=device) * num_t\n",
        "\n",
        "            model_out = self.model(torch.cat([x] * 2), torch.cat([t * 999] * 2), cond)\n",
        "            noise_pred_uncond, noise_pred_text = model_out.chunk(2)\n",
        "            pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            sigma_t = self.sigma_t(num_t)\n",
        "            pred_sigma = pred + (sigma_t**2) / (2 * (self.noise_scale**2) * ((1.0 - num_t) ** 2)) * (\n",
        "                0.5 * num_t * (1.0 - num_t) * pred - 0.5 * (2.0 - num_t) * x.detach().clone()\n",
        "            )\n",
        "\n",
        "            x = x.detach().clone() + pred_sigma * dt + sigma_t * np.sqrt(dt) * torch.randn_like(pred_sigma).to(device)\n",
        "\n",
        "        return x, self.sample_N"
      ],
      "metadata": {
        "id": "2TZyCbPplciH"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train module"
      ],
      "metadata": {
        "id": "0W819YWdTzCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### scvi encoding"
      ],
      "metadata": {
        "id": "21dEMxSOw1ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import anndata\n",
        "import scvi\n",
        "import os\n",
        "\n",
        "file_count = 0\n",
        "\n",
        "adata_list = []\n",
        "folder_path = '/content/drive/MyDrive/tf-flow-design/combined_adata_folder/'\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.h5ad'):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        print(filename)\n",
        "        adata = anndata.read_h5ad(file_path)\n",
        "        adata_list.append(adata)\n",
        "        file_count += 1\n",
        "        if file_count >= 5:\n",
        "            break\n",
        "\n",
        "print(f'Read and stored {file_count} .h5ad files.')"
      ],
      "metadata": {
        "id": "_KZ5OfT8kPNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d579d850-9734-401b-f1ea-022589fbc805"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combined_adata_macrophage.h5ad\n",
            "combined_adata_monocyte.h5ad\n",
            "combined_adata_endothelial cell of hepatic sinusoid.h5ad\n",
            "combined_adata_liver dendritic cell.h5ad\n",
            "combined_adata_nk cell.h5ad\n",
            "Read and stored 5 .h5ad files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Concatenate all AnnData objects\n",
        "# combined_adata = anndata.concat(adata_list, join='outer', label='batch')\n",
        "# print(f\"Combined AnnData shape: {combined_adata.shape}\")"
      ],
      "metadata": {
        "id": "78QR-aHoi5zC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import anndata\n",
        "\n",
        "adata_dict = {}\n",
        "\n",
        "for adata in adata_list:\n",
        "    # Get the unique cell_ontology_class values (excluding 'mesenchymal stem cell')\n",
        "    cell_types = adata.obs['cell_ontology_class'].unique()\n",
        "    for cell_type in cell_types:\n",
        "        if cell_type != 'mesenchymal stem cell':\n",
        "            if cell_type not in adata_dict:\n",
        "                adata_dict[cell_type] = adata\n",
        "            else:\n",
        "                adata_dict[cell_type] = anndata.concat([adata_dict[cell_type], adata])\n"
      ],
      "metadata": {
        "id": "Lts1Qur2av3d"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adata_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxoZTz7QQaP9",
        "outputId": "025d2789-944d-44b6-dfb0-fc3efcd7a5c6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['macrophage', 'monocyte', 'endothelial cell of hepatic sinusoid', 'liver dendritic cell', 'nk cell'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scvi_encoder = SCVIEncodingModule()\n",
        "scvi_latents, scvi_pseudotimes = scvi_encoder.encode(adata_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8zBTpfQQ5KW",
        "outputId": "f01a00c7-4c79-4d95-9729-e9836c3e3547"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and embedding for cell type: macrophage...\n",
            "No NaN values found in adata_copy.X for cell type: macrophage\n",
            "Training and embedding for cell type: monocyte...\n",
            "No NaN values found in adata_copy.X for cell type: monocyte\n",
            "Training and embedding for cell type: endothelial cell of hepatic sinusoid...\n",
            "No NaN values found in adata_copy.X for cell type: endothelial cell of hepatic sinusoid\n",
            "Training and embedding for cell type: liver dendritic cell...\n",
            "No NaN values found in adata_copy.X for cell type: liver dendritic cell\n",
            "Training and embedding for cell type: nk cell...\n",
            "No NaN values found in adata_copy.X for cell type: nk cell\n",
            "Encoding completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scvi_latents['macrophage'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_xhzSQ7v5uU",
        "outputId": "94034909-514a-49e7-8c16-c7f481ac148c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50663, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scvi_pseudotimes['macrophage'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37G0qSqw5mve",
        "outputId": "6d3f7e57-300e-47ea-b81c-727ef79b767c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50663,)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### generate random protein sequences for each scvi latent (3 pos, 10 neg)"
      ],
      "metadata": {
        "id": "JsBKL442wzXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# valid AAs\n",
        "valid_amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
        "\n",
        "def generate_random_protein_sequence(min_length=100, max_length=200):\n",
        "    length = random.randint(min_length, max_length)\n",
        "    return ''.join(random.choices(valid_amino_acids, k=length))\n",
        "\n",
        "protein_sequences = {}\n",
        "\n",
        "for cell_type in scvi_latents.keys():\n",
        "    protein_sequences[cell_type] = [generate_random_protein_sequence() for _ in range(3)]\n",
        "\n",
        "cell_type_example = 'macrophage'\n",
        "print(f\"prot seqs for {cell_type_example}:\")\n",
        "for seq in protein_sequences[cell_type_example]:\n",
        "    print(seq)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG1QHIYTw7lL",
        "outputId": "a6ca9210-8e3f-441c-ab40-b94db0fb1fd0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prot seqs for macrophage:\n",
            "RGAFKLTNWGNESMPYEAQCVLFLYEHYYTKTRIRSFNRRLLEFIWVWYARMENQLINDYVPMVIRRTIDPPFLRSEMYFFWVNDQACHQNKGYFRSGMWMEGAKMKIC\n",
            "TDCSSDLDAKMEYTKDGEHPENLTDMKHAIKDRGPKVRCYWSCCFWMFESGRLAYQQTMFGNGCFINTAWMHPFGGYCYDQWFAHLAQARWWRMLPGYTAGIKFMMVQFDPWYMIMTLDNAIVQHYCHAHPSPVWVSIWISNHQNHTYWFNVNLYCGFAADNEREKSDLPMKKFFANIH\n",
            "GNHWAKHKHSIHNDVDDYKTDPWECHSFQPMGSHLLMRSWREKTEAMIWTTYISHTNCEPWCQWKSSFSFGWDRRDAFFGWTNYRICITCLAEMRVQQGSVCAKEENDTNSCMKYCVKTRGHC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### datasets"
      ],
      "metadata": {
        "id": "lAAHts2YyYxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "1xMZflIi0wBg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = ProtT5EncodingModule()\n",
        "\n",
        "latent_list = []\n",
        "pseudotime_list = []\n",
        "sequence_list = []\n",
        "\n",
        "for cell_type, latents in scvi_latents.items():\n",
        "    pseudotimes = scvi_pseudotimes[cell_type]\n",
        "    for latent, pseudotime, sequence in zip(latents, pseudotimes, protein_sequences[cell_type]):\n",
        "        print(f\"Cell type: {cell_type}, Latent shape: {latents.shape}, Sequence length: {len(sequence)}, Pseudotime shape: {pseudotimes.shape}\")\n",
        "        latent_list.append(latents)\n",
        "        pseudotime_list.append(pseudotimes)\n",
        "        sequence_list.append(sequence)\n",
        "\n",
        "# seqs encoding protT5\n",
        "sequence_tensor_list = []\n",
        "for sequence in sequence_list:\n",
        "    sequence_tensor = encoder(sequence)\n",
        "    sequence_tensor_list.append(sequence_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc9X0I6-0YQq",
        "outputId": "4b90af83-160f-428e-94ed-80edfdea6baf"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell type: macrophage, Latent shape: (50663, 50), Sequence length: 109, Pseudotime shape: (50663,)\n",
            "Cell type: macrophage, Latent shape: (50663, 50), Sequence length: 179, Pseudotime shape: (50663,)\n",
            "Cell type: macrophage, Latent shape: (50663, 50), Sequence length: 123, Pseudotime shape: (50663,)\n",
            "Cell type: monocyte, Latent shape: (27973, 50), Sequence length: 143, Pseudotime shape: (27973,)\n",
            "Cell type: monocyte, Latent shape: (27973, 50), Sequence length: 173, Pseudotime shape: (27973,)\n",
            "Cell type: monocyte, Latent shape: (27973, 50), Sequence length: 153, Pseudotime shape: (27973,)\n",
            "Cell type: endothelial cell of hepatic sinusoid, Latent shape: (15880, 50), Sequence length: 187, Pseudotime shape: (15880,)\n",
            "Cell type: endothelial cell of hepatic sinusoid, Latent shape: (15880, 50), Sequence length: 131, Pseudotime shape: (15880,)\n",
            "Cell type: endothelial cell of hepatic sinusoid, Latent shape: (15880, 50), Sequence length: 159, Pseudotime shape: (15880,)\n",
            "Cell type: liver dendritic cell, Latent shape: (15493, 50), Sequence length: 126, Pseudotime shape: (15493,)\n",
            "Cell type: liver dendritic cell, Latent shape: (15493, 50), Sequence length: 183, Pseudotime shape: (15493,)\n",
            "Cell type: liver dendritic cell, Latent shape: (15493, 50), Sequence length: 100, Pseudotime shape: (15493,)\n",
            "Cell type: nk cell, Latent shape: (24315, 50), Sequence length: 162, Pseudotime shape: (24315,)\n",
            "Cell type: nk cell, Latent shape: (24315, 50), Sequence length: 122, Pseudotime shape: (24315,)\n",
            "Cell type: nk cell, Latent shape: (24315, 50), Sequence length: 137, Pseudotime shape: (24315,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_tensor_list[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g6OvPn_1Dpx",
        "outputId": "9ff37537-7580-4faf-a89b-532523b69fba"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 110, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# pad\n",
        "max_len = max(sequence.shape[1] for sequence in sequence_tensor_list)\n",
        "padded_sequence_tensor_list = [\n",
        "    torch.cat([sequence, torch.zeros(1, max_len - sequence.shape[1], sequence.shape[2], dtype=sequence.dtype)], dim=1)\n",
        "    if sequence.shape[1] < max_len else sequence for sequence in sequence_tensor_list\n",
        "]\n",
        "sequence_tensor = torch.cat(padded_sequence_tensor_list, dim=0)\n"
      ],
      "metadata": {
        "id": "Bqj90ko108cF"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vUDzJ6h3g0d",
        "outputId": "03fe51bb-c595-45f6-fd47-15d49f4d4b59"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15, 188, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_list[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7_YDnHv6eYw",
        "outputId": "7be60722-01ab-442f-b9e7-d19e36c39a78"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50663, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_tensor_list = [torch.tensor(latent, dtype=torch.float32) for latent in latent_list]\n",
        "\n",
        "max_len = max(latent.shape[0] for latent in latent_tensor_list)\n",
        "\n",
        "padded_latent_list = [\n",
        "    torch.cat([latent, torch.zeros((max_len - latent.shape[0], latent.shape[1]), dtype=latent.dtype)], dim=0)\n",
        "    if latent.shape[0] < max_len else latent for latent in latent_tensor_list\n",
        "]\n",
        "\n",
        "latent_tensor = torch.stack(padded_latent_list, dim=0)\n"
      ],
      "metadata": {
        "id": "Funv8saA2pIG"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_tensor_list[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1W8clBx8Hhl",
        "outputId": "1d652f82-e30f-40c1-85a5-e22ae832b544"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50663, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvUO7Z3K3xES",
        "outputId": "2985a70e-5338-4360-bc86-d694617b2c3c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15, 50663, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### pseudotime as a context:\n",
        "adata_list[0].obs['dpt_pseudotime'].shape\n",
        "unique_counts = adata_list[0].obs['cell_ontology_class'].value_counts()\n",
        "print(unique_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS86ezG-3Rns",
        "outputId": "3577b289-8882-451a-f3be-58d3da716e18"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cell_ontology_class\n",
            "macrophage               35204\n",
            "mesenchymal stem cell    15459\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pseudotime_tensor_list = [torch.tensor(pseudotime, dtype=torch.float32) for pseudotime in pseudotime_list]\n",
        "\n",
        "# ensure pseudotime tensors have shape (seqlen, 1) before padding\n",
        "pseudotime_tensor_list = [\n",
        "    pseudotime if len(pseudotime.shape) == 2 else pseudotime.unsqueeze(1) for pseudotime in pseudotime_tensor_list\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvKatCSV70-m",
        "outputId": "6dc429a5-db60-4130-ed5a-3f78432eb794"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-f91bad088d9a>:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  pseudotime_tensor_list = [torch.tensor(pseudotime, dtype=torch.float32) for pseudotime in pseudotime_list]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pseudotime_tensor_list[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_Y_yA808L6D",
        "outputId": "8cafff65-1eb7-40ed-91c9-c1a2d6677622"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50663, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_pseudotime_len = max(pseudotime.shape[0] for pseudotime in pseudotime_tensor_list)\n",
        "\n",
        "padded_pseudotime_tensor_list = [\n",
        "    torch.cat([pseudotime, torch.zeros((max_pseudotime_len - pseudotime.shape[0], pseudotime.shape[1]), dtype=pseudotime.dtype)], dim=0)\n",
        "    if pseudotime.shape[0] < max_pseudotime_len else pseudotime for pseudotime in pseudotime_tensor_list\n",
        "]\n",
        "\n",
        "pseudotime_tensor = torch.stack(padded_pseudotime_tensor_list, dim=0)"
      ],
      "metadata": {
        "id": "NHiB_2nk8Fig"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pseudotime_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlYN6aGh84XI",
        "outputId": "aa2129ec-960c-482f-a3f1-3ea9b718d9e5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15, 50663, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seq dim = 1024, latent dim = 50, pseudotime dim = 1"
      ],
      "metadata": {
        "id": "vGgHwRuM-Rdf"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "full_dataset = TensorDataset(latent_tensor, pseudotime_tensor, sequence_tensor)\n",
        "\n",
        "# split size calculation\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(0.7 * total_size)\n",
        "val_size = total_size - train_size\n",
        "\n",
        "# dataset split\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# dataloader creation\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "kdqzZHCk_HWc"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEu3MrI3_SC3",
        "outputId": "2f5e3c4b-1637-4310-dc64-2fcb3a45231a"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7975e3ab7100>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_dataloader:\n",
        "    scvi, dpt, seq = batch\n",
        "    print(scvi.shape)\n",
        "    print(seq.shape)\n",
        "    print(dpt.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2D220eTz-0L",
        "outputId": "bc01d7fd-f093-4e6a-cda1-e548642b1cd5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 50663, 50])\n",
            "torch.Size([1, 188, 1024])\n",
            "torch.Size([1, 50663, 1])\n",
            "torch.Size([1, 50663, 50])\n",
            "torch.Size([1, 188, 1024])\n",
            "torch.Size([1, 50663, 1])\n",
            "torch.Size([1, 50663, 50])\n",
            "torch.Size([1, 188, 1024])\n",
            "torch.Size([1, 50663, 1])\n",
            "torch.Size([1, 50663, 50])\n",
            "torch.Size([1, 188, 1024])\n",
            "torch.Size([1, 50663, 1])\n",
            "torch.Size([1, 50663, 50])\n",
            "torch.Size([1, 188, 1024])\n",
            "torch.Size([1, 50663, 1])\n",
            "torch.Size([1, 50663, 50])\n",
            "torch.Size([1, 188, 1024])\n",
            "torch.Size([1, 50663, 1])\n",
            "torch.Size([1, 50663, 50])\n",
            "torch.Size([1, 188, 1024])\n",
            "torch.Size([1, 50663, 1])\n",
            "torch.Size([1, 50663, 50])\n",
            "torch.Size([1, 188, 1024])\n",
            "torch.Size([1, 50663, 1])\n",
            "torch.Size([1, 50663, 50])\n",
            "torch.Size([1, 188, 1024])\n",
            "torch.Size([1, 50663, 1])\n",
            "torch.Size([1, 50663, 50])\n",
            "torch.Size([1, 188, 1024])\n",
            "torch.Size([1, 50663, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### models"
      ],
      "metadata": {
        "id": "RpCeq3tdyZv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProteinFlowMatching(nn.Module):\n",
        "    def __init__(self, flow_matching, decoder):\n",
        "        super().__init__()\n",
        "        self.flow_matching = flow_matching\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        # During training, we only need to compute the loss from flow matching\n",
        "        return self.flow_matching(x, context)\n",
        "\n",
        "    def generate(self, x, context, num_steps=200):\n",
        "        # Generate latent representation using flow matching\n",
        "        latent = self.flow_matching.euler_sample(context, x.shape, guidance_scale=3.0)[0]\n",
        "\n",
        "        # Decode the latent representation to protein sequence\n",
        "        protein_sequence = self.decoder(latent, max_length=num_steps)\n",
        "\n",
        "        return protein_sequence"
      ],
      "metadata": {
        "id": "VwYmnDMhLbdO"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "unet = CustomUNet1D(\n",
        "    in_channels=50,  # Dimension of scVI latents\n",
        "    out_channels=1024,  # Dimension of protein embeddings\n",
        "    model_channels=64,\n",
        "    num_res_blocks=2,\n",
        "    attention_resolutions=(1,),\n",
        "    dropout=0.1,\n",
        "    channel_mult=(1, 2, 4, 8),\n",
        "    use_spatial_transformer=True,\n",
        "    transformer_depth=1,\n",
        "    context_dim=1,  # Dimension of pseudotime\n",
        ")\n",
        "flow_matching = FlowMatchingTrainer(unet, sample_N=25)\n",
        "decoder = ProtT5DecodingModule()  # vocab_size is the number of amino acids + special tokens\n",
        "model = ProteinFlowMatching(flow_matching, decoder)\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oRb8_B9wLnH",
        "outputId": "ad6bc2a1-13be-4a52-ffde-362a602ea871"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ProteinFlowMatching(\n",
              "  (flow_matching): FlowMatchingTrainer(\n",
              "    (model): CustomUNet1D(\n",
              "      (time_embed): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): SiLU()\n",
              "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (input_blocks): ModuleList(\n",
              "        (0): Conv1d(50, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (1-2): 2 x Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=64, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Identity()\n",
              "          )\n",
              "          (1): SpatialTransformer1D(\n",
              "            (norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
              "            (proj_in): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
              "            (transformer_blocks): ModuleList(\n",
              "              (0): BasicTransformerBlock(\n",
              "                (attn1): CrossAttention(\n",
              "                  (to_q): Linear(in_features=64, out_features=512, bias=False)\n",
              "                  (to_k): Linear(in_features=64, out_features=512, bias=False)\n",
              "                  (to_v): Linear(in_features=64, out_features=512, bias=False)\n",
              "                  (to_out): Linear(in_features=512, out_features=64, bias=True)\n",
              "                )\n",
              "                (ff): FeedForward(\n",
              "                  (net): Sequential(\n",
              "                    (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "                    (1): GELU(approximate='none')\n",
              "                    (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "                  )\n",
              "                )\n",
              "                (attn2): CrossAttention(\n",
              "                  (to_q): Linear(in_features=64, out_features=512, bias=False)\n",
              "                  (to_k): Linear(in_features=1, out_features=512, bias=False)\n",
              "                  (to_v): Linear(in_features=1, out_features=512, bias=False)\n",
              "                  (to_out): Linear(in_features=512, out_features=64, bias=True)\n",
              "                )\n",
              "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "                (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "            )\n",
              "            (proj_out): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (3): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "        (4): Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (5): Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Identity()\n",
              "          )\n",
              "        )\n",
              "        (6): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "        (7): Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (8): Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Identity()\n",
              "          )\n",
              "        )\n",
              "        (9): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "        (10): Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=512, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (11): Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=512, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Identity()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (middle_block): Sequential(\n",
              "        (0): ResBlock1D(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.1, inplace=False)\n",
              "            (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "          )\n",
              "          (skip_connection): Identity()\n",
              "        )\n",
              "        (1): SpatialTransformer1D(\n",
              "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "          (proj_in): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): BasicTransformerBlock(\n",
              "              (attn1): CrossAttention(\n",
              "                (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "                  (1): GELU(approximate='none')\n",
              "                  (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): CrossAttention(\n",
              "                (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "                (to_k): Linear(in_features=1, out_features=512, bias=False)\n",
              "                (to_v): Linear(in_features=1, out_features=512, bias=False)\n",
              "                (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
              "              )\n",
              "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (proj_out): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "        )\n",
              "        (2): ResBlock1D(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.1, inplace=False)\n",
              "            (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "          )\n",
              "          (skip_connection): Identity()\n",
              "        )\n",
              "      )\n",
              "      (output_blocks): ModuleList(\n",
              "        (0-1): 2 x Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=512, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (2): Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 768, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(768, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=512, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Conv1d(768, 512, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "          (1): ConvTranspose1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 768, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(768, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Conv1d(768, 256, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (4): Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(512, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (5): Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Conv1d(384, 256, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "          (1): ConvTranspose1d(256, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
              "        )\n",
              "        (6): Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(384, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Conv1d(384, 128, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (7): Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (8): Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 192, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(192, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Conv1d(192, 128, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "          (1): ConvTranspose1d(128, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
              "        )\n",
              "        (9): Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 192, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(192, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=64, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Conv1d(192, 64, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "          (1): SpatialTransformer1D(\n",
              "            (norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
              "            (proj_in): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
              "            (transformer_blocks): ModuleList(\n",
              "              (0): BasicTransformerBlock(\n",
              "                (attn1): CrossAttention(\n",
              "                  (to_q): Linear(in_features=64, out_features=512, bias=False)\n",
              "                  (to_k): Linear(in_features=64, out_features=512, bias=False)\n",
              "                  (to_v): Linear(in_features=64, out_features=512, bias=False)\n",
              "                  (to_out): Linear(in_features=512, out_features=64, bias=True)\n",
              "                )\n",
              "                (ff): FeedForward(\n",
              "                  (net): Sequential(\n",
              "                    (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "                    (1): GELU(approximate='none')\n",
              "                    (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "                  )\n",
              "                )\n",
              "                (attn2): CrossAttention(\n",
              "                  (to_q): Linear(in_features=64, out_features=512, bias=False)\n",
              "                  (to_k): Linear(in_features=1, out_features=512, bias=False)\n",
              "                  (to_v): Linear(in_features=1, out_features=512, bias=False)\n",
              "                  (to_out): Linear(in_features=512, out_features=64, bias=True)\n",
              "                )\n",
              "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "                (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "            )\n",
              "            (proj_out): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "        (10-11): 2 x Sequential(\n",
              "          (0): ResBlock1D(\n",
              "            (in_layers): Sequential(\n",
              "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (emb_layers): Sequential(\n",
              "              (0): SiLU()\n",
              "              (1): Linear(in_features=256, out_features=64, bias=True)\n",
              "            )\n",
              "            (out_layers): Sequential(\n",
              "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
              "              (1): SiLU()\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "            )\n",
              "            (skip_connection): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "          (1): SpatialTransformer1D(\n",
              "            (norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
              "            (proj_in): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
              "            (transformer_blocks): ModuleList(\n",
              "              (0): BasicTransformerBlock(\n",
              "                (attn1): CrossAttention(\n",
              "                  (to_q): Linear(in_features=64, out_features=512, bias=False)\n",
              "                  (to_k): Linear(in_features=64, out_features=512, bias=False)\n",
              "                  (to_v): Linear(in_features=64, out_features=512, bias=False)\n",
              "                  (to_out): Linear(in_features=512, out_features=64, bias=True)\n",
              "                )\n",
              "                (ff): FeedForward(\n",
              "                  (net): Sequential(\n",
              "                    (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "                    (1): GELU(approximate='none')\n",
              "                    (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "                  )\n",
              "                )\n",
              "                (attn2): CrossAttention(\n",
              "                  (to_q): Linear(in_features=64, out_features=512, bias=False)\n",
              "                  (to_k): Linear(in_features=1, out_features=512, bias=False)\n",
              "                  (to_v): Linear(in_features=1, out_features=512, bias=False)\n",
              "                  (to_out): Linear(in_features=512, out_features=64, bias=True)\n",
              "                )\n",
              "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "                (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "            )\n",
              "            (proj_out): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (out): Sequential(\n",
              "        (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
              "        (1): SiLU()\n",
              "        (2): Conv1d(64, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): ProtT5DecodingModule(\n",
              "    (protT5_model): T5ForConditionalGeneration(\n",
              "      (shared): Embedding(128, 1024)\n",
              "      (encoder): T5Stack(\n",
              "        (embed_tokens): Embedding(128, 1024)\n",
              "        (block): ModuleList(\n",
              "          (0): T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                  (relative_attention_bias): Embedding(32, 32)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseActDense(\n",
              "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
              "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): ReLU()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1-23): 23 x T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseActDense(\n",
              "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
              "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): ReLU()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): T5LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (decoder): T5Stack(\n",
              "        (embed_tokens): Embedding(128, 1024)\n",
              "        (block): ModuleList(\n",
              "          (0): T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                  (relative_attention_bias): Embedding(32, 32)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerCrossAttention(\n",
              "                (EncDecAttention): T5Attention(\n",
              "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (2): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseActDense(\n",
              "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
              "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): ReLU()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1-23): 23 x T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerCrossAttention(\n",
              "                (EncDecAttention): T5Attention(\n",
              "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (2): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseActDense(\n",
              "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
              "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): ReLU()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): T5LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=1024, out_features=128, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train"
      ],
      "metadata": {
        "id": "yVlSfboYplb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for scvi_latent, pseudotime, protein_seq in train_dataloader:\n",
        "        scvi_latent, pseudotime, protein_seq = scvi_latent.to(device), pseudotime.to(device), protein_seq.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = flow_matching(scvi_latent, pseudotime)  # Using pseudotime as context\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for scvi_latent, pseudotime, protein_seq in val_dataloader:\n",
        "            scvi_latent, pseudotime, protein_seq = scvi_latent.to(device), pseudotime.to(device), protein_seq.to(device)\n",
        "            loss = flow_matching(scvi_latent, pseudotime)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Generate new protein sequences\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    scvi_latent = torch.randn(2000, 50).to(device)  # Random scVI latent\n",
        "    pseudotime = torch.rand(2000, 1).to(device)  # Random pseudotime\n",
        "    generated_sequence = model.generate(scvi_latent, pseudotime)\n",
        "    print(\"Generated sequence:\", generated_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BCS9pXSrzHrB",
        "outputId": "5776dcf9-af69-41a3-b918-4fb9bcec6612"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Module type input blocks: <class 'torch.nn.modules.conv.Conv1d'>\n",
            "Module type input blocks: <class 'torch.nn.modules.container.Sequential'>\n",
            "submodule type input blocks: <class '__main__.ResBlock1D'>\n",
            "submodule type input blocks: <class '__main__.SpatialTransformer1D'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 76.50 GiB. GPU ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-75850e41fd5a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflow_matching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscvi_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpseudotime\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Using pseudotime as context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-096c1af17a9c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_0, c)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mperturbed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_expand\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_expand\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mmodel_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperturbed_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"none\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-1aae1be4ab5f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, timesteps, context)\u001b[0m\n\u001b[1;32m    145\u001b[0m                       \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpatialTransformer1D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                       \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m                   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                       \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-1aae1be4ab5f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-1aae1be4ab5f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-1aae1be4ab5f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bhid,bhjd->bhij'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bhij,bhjd->bhid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# the path for contracting 0 or 1 time(s) is already optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;31m# or the user has disabled using opt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 76.50 GiB. GPU "
          ]
        }
      ]
    }
  ]
}